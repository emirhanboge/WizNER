{
    "id": "21785991",
    "revid": "28481209",
    "url": "https://en.wikipedia.org/wiki?curid=21785991",
    "title": "Adjusted mutual information",
    "text": "In &lt;a href=\"probability%20theory\"&gt;probability theory&lt;/a&gt; and &lt;a href=\"information%20theory\"&gt;information theory&lt;/a&gt;, adjusted mutual information, a variation of &lt;a href=\"mutual%20information\"&gt;mutual information&lt;/a&gt; may be used for comparing &lt;a href=\"Cluster_Analysis\"&gt;clusterings&lt;/a&gt;. It corrects the effect of agreement solely due to chance between clusterings, similar to the way the &lt;a href=\"adjusted%20rand%20index\"&gt;adjusted rand index&lt;/a&gt; corrects the &lt;a href=\"Rand%20index\"&gt;Rand index&lt;/a&gt;. It is closely related to &lt;a href=\"variation%20of%20information\"&gt;variation of information&lt;/a&gt;: when a similar adjustment is made to the VI index, it becomes equivalent to the AMI. The adjusted measure however is no longer metrical.\nMutual information of two partitions.\nGiven a set \"S\" of \"N\" elements formula_1, consider two &lt;a href=\"partition%20of%20a%20set\"&gt;partitions&lt;/a&gt; of \"S\", namely formula_2 with \"R\" clusters, and formula_3 with \"C\" clusters. It is presumed here that the partitions are so-called \"hard clusters;\" the partitions are pairwise disjoint:\nfor all formula_5, and complete:\nThe &lt;a href=\"mutual%20information\"&gt;mutual information&lt;/a&gt; of cluster overlap between \"U\" and \"V\" can be summarized in the form of an \"R\"x\"C\" &lt;a href=\"contingency%20table\"&gt;contingency table&lt;/a&gt; formula_7, where formula_8 denotes the number of objects that are common to clusters formula_9 and formula_10. That is,\nSuppose an object is picked at random from \"S\"; the probability that the object falls into cluster formula_9 is:\nThe &lt;a href=\"Entropy_%28information_theory%29\"&gt;entropy&lt;/a&gt; associated with the partitioning \"U\" is:\n\"H(U)\" is non-negative and takes the value 0 only when there is no uncertainty determining an object's cluster membership, \"i.e.\", when there is only one cluster. Similarly, the entropy of the clustering \"V\" can be calculated as:\nwhere formula_16. The &lt;a href=\"mutual%20information\"&gt;mutual information&lt;/a&gt; (MI) between two partitions:\nwhere formula_18 denotes the probability that a point belongs to both the cluster formula_9 in \"U\" and cluster formula_10 in \"V\":\nMI is a non-negative quantity upper bounded by the entropies \"H\"(\"U\") and \"H\"(\"V\"). It quantifies the information shared by the two clusterings and thus can be employed as a clustering &lt;a href=\"similarity%20measure\"&gt;similarity measure&lt;/a&gt;.\nAdjustment for chance.\nLike the &lt;a href=\"Rand%20index\"&gt;Rand index&lt;/a&gt;, the baseline value of mutual information between two random clusterings does not take on a constant value, and tends to be larger when the two partitions have a larger number of clusters (with a fixed number of set elements \"N\").\nBy adopting a &lt;a href=\"Hypergeometric%20distribution\"&gt;hypergeometric&lt;/a&gt; model of randomness, it can be shown that the expected mutual information between two random clusterings is:\nwhere formula_23\ndenotes formula_24. The variables formula_25 and formula_26 are partial sums of the contingency table; that is,\nand\nThe adjusted measure for the mutual information may then be defined to be:\nThe AMI takes a value of 1 when the two partitions are identical and 0 when the MI between two partitions equals the value expected due to chance alone."
}