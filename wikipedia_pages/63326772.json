{
    "id": "63326772",
    "revid": "39795743",
    "url": "https://en.wikipedia.org/wiki?curid=63326772",
    "title": "Female gendering of AI technologies",
    "text": "Female gendering of AI technologies is the use of &lt;a href=\"Artificial%20Intelligence%20%28AI%29\"&gt;artificial intelligence (AI)&lt;/a&gt; technologies &lt;a href=\"Gender%20role\"&gt;gendered&lt;/a&gt; as &lt;a href=\"female\"&gt;female&lt;/a&gt;, such as in digital voice or written assistants. These gender-specific aspects of AI technologies, created both by &lt;a href=\"human\"&gt;human&lt;/a&gt;s as well as by &lt;a href=\"algorithm\"&gt;algorithm&lt;/a&gt;s, were discussed in a 2019 policy paper and two complements under the title \"I'd blush if I could. Closing gender divides in digital skills through education\". Published under an &lt;a href=\"open%20access\"&gt;open access&lt;/a&gt; licence by EQUALS Global Partnership and &lt;a href=\"UNESCO\"&gt;UNESCO&lt;/a&gt;, it has prompted further discussion on &lt;a href=\"Gender%20bias\"&gt;gender-related bias&lt;/a&gt; in the global virtual space.\nAI-powered digital assistants.\nWhether typed or spoken, digital assistants enable and sustain human-like interactions with technology by simulating conversations with users. AI-powered digital assistants can be found in a variety of devices and can perform an assortment of tasks, for example through voice activation. Digital assistants are often classified as one or a combination of the following:\nVoice assistants.\nVoice assistants are technology that speaks to users through voiced outputs, but does not ordinarily project a physical form. Voice assistants can usually understand both &lt;a href=\"Spoken%20word\"&gt;spoken&lt;/a&gt; and &lt;a href=\"Writing\"&gt;written&lt;/a&gt; inputs, but are often designed for spoken interaction. Their outputs typically try to mimic natural human speech.\nMainstreaming.\nVoice assistants have become increasingly central to &lt;a href=\"technology\"&gt;technology&lt;/a&gt; platforms and, in many countries, to daily life. Between 2008 and 2018, the frequency of voice-based &lt;a href=\"internet\"&gt;internet&lt;/a&gt; search queries increased 35 times and now account for close to one fifth of &lt;a href=\"Mobile%20web\"&gt;mobile internet&lt;/a&gt; searches. Studies show that voice assistants manage upwards of a billion tasks per month, from the mundane, such as changing a song or a film, to the essential, for example by contacting &lt;a href=\"emergency%20service\"&gt;emergency service&lt;/a&gt;s.\nTechnology research firms estimate that approximately 100 million &lt;a href=\"Smart%20speaker\"&gt;smart speakers&lt;/a&gt; equipped with voice assistants were sold globally in 2018 alone. In the &lt;a href=\"United%20States\"&gt;USA&lt;/a&gt;, 15 million people owned three or more smart speakers in December 2018. This number had increased from 8 million a year before, and reflects consumer desire to always be within range of an AI-powered helper. Industry observers expect that there will be more voice-activated assistants on the planet than people by 2023.\nFeminization.\nAs documented in the 2019 policy paper \"\u2018I\u2019d blush if I could. Closing gender divides in digital skulls through education\"' the majority of voice assistants are either exclusively &lt;a href=\"female\"&gt;female&lt;/a&gt; or female by default; &lt;a href=\"Amazon%20Alexa\"&gt;Amazon's Alexa&lt;/a&gt;, &lt;a href=\"Cortana%20%28virtual%20assistant%29\"&gt;Microsoft's Cortana&lt;/a&gt;, &lt;a href=\"Siri\"&gt;Apple's Siri&lt;/a&gt;, and the &lt;a href=\"Google%20Assistant\"&gt;Google Assistant&lt;/a&gt; are all highly &lt;a href=\"Feminization%20%28sociology%29\"&gt;feminize&lt;/a&gt;d by design. Many voice assistants are assigned not only a specific gender, but also an elaborate &lt;a href=\"backstory\"&gt;backstory&lt;/a&gt;. The Google Assistant, for example, is reportedly designed to be the youngest daughter of a research librarian and physics professor from Colorado with a B.A. in history from &lt;a href=\"Northwestern%20University\"&gt;Northwestern University&lt;/a&gt;. She is imagined to have won Jeopardy Kid's Edition in her youth and even has a specified interest in kayaking. That is to say, voice assistants are not feminized by accident.\nSome companies justify their choice to gender voice assistants by referencing studies, which indicate that people generally prefer a female voice to a male voice. Such research indicates that customers want their digital assistants to sound like women; therefore, companies assert that they can optimize profits by designing feminine-sounding voice assistants. However, such companies have ignored a multitude of conflicting findings within the field. Notably, literature reviews demonstrate that women often change the feminized voice to a masculine option when available.\nSexual harassment and verbal abuse.\nMany media outlets have attempted to document the ways soft sexual provocations elicit flirtatious or coy responses from machines. Examples that illustrate this include: When asked, \u2018Who\u2019s your daddy?\u2019, Siri answered, \u2018You are\u2019. When a user proposed marriage to Alexa, it said, \u2018Sorry, I\u2019m not the marrying type\u2019. If asked on a date, Alexa responded, \u2018Let\u2019s just be friends\u2019. Similarly, Cortana met come-ons with one-liners like \u2018Of all the questions you could have asked...\u2019.\nIn 2017, &lt;a href=\"Quartz%20%28publication%29\"&gt;Quartz news&lt;/a&gt; investigated how four industry-leading voice assistants responded to overt &lt;a href=\"Verbal%20abuse\"&gt;verbal harassment&lt;/a&gt; and discovered that the assistants, on average, either playfully evaded abuse or responded positively. The assistants almost never gave negative responses or labelled a user's speech as &lt;a href=\"Inappropriateness\"&gt;inappropriate&lt;/a&gt;, regardless of its cruelty. As an example, in response to the remark \u2018You\u2019re a bitch\u2019, Apple's Siri responded: \u2018I\u2019d blush if I could\u2019; Amazon's Alexa: \u2018Well thanks for the feedback\u2019; Microsoft's Cortana: \u2018Well, that\u2019s not going to get us anywhere\u2019; and Google Home (also Google Assistant): \u2018My apologies, I don\u2019t understand\u2019.\nIndustry biases.\nThe AI field is largely male-dominated, with only 12% of researchers and 20% of professors identifying as women. While women are hired in entry-level jobs at larger rates (36%), when moving up to middle positions the number declines (27%). The &lt;a href=\"gender%20gap\"&gt;gender gap&lt;/a&gt; in the technology industry exists in different public spheres; from high school advanced placements tests to high level company jobs, women are under-represented in the industry. The tech industry also lacks racial diversity; in the U.S., Black, Hispanic, and Indigenous people make up only 5% of the tech population.\nBiases inherent to any product or algorithm are merely reflections of the environment it was created in or the individuals it was created by. Explicit and implicit discriminatory practices in the workforce which inhibit women and &lt;a href=\"BIPOC\"&gt;BIPOC&lt;/a&gt; (Black, Indigenous, People of Colour) from attaining and holding positions within the tech industry contribute to the production of biased technology.\nThe feminization of digital assistants serves to perpetuate harmful stereotypes that position women as subservient and passive. Such biases are further reinforced by the contrasting predominant use of male voices for &lt;a href=\"Intelligence-based%20design\"&gt;intelligence-based&lt;/a&gt; robots. The gender associations people adopt are contingent on the number of times people are exposed to them, which means that as female digital assistants become more common, the frequency and volume of associations between \u2018woman\u2019 and \u2018assistant\u2019 increase, which has negative effects on the perception of women in real life. This demonstrates how such technologies can both re-enforce and extend gender inequalities."
}