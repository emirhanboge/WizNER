{
    "id": "331258",
    "revid": "29986787",
    "url": "https://en.wikipedia.org/wiki?curid=331258",
    "title": "Moore\u2013Penrose inverse",
    "text": "In &lt;a href=\"mathematics\"&gt;mathematics&lt;/a&gt;, and in particular &lt;a href=\"linear%20algebra\"&gt;linear algebra&lt;/a&gt;, the Moore\u2013Penrose inverse of a &lt;a href=\"matrix%20%28mathematics%29\"&gt;matrix&lt;/a&gt; is the most widely known &lt;a href=\"generalization\"&gt;generalization&lt;/a&gt; of the &lt;a href=\"inverse%20matrix\"&gt;inverse matrix&lt;/a&gt;. It was independently described by &lt;a href=\"E.%20H.%20Moore\"&gt;E. H. Moore&lt;/a&gt; in 1920, &lt;a href=\"Arne%20Bjerhammar\"&gt;Arne Bjerhammar&lt;/a&gt; in 1951, and &lt;a href=\"Roger%20Penrose\"&gt;Roger Penrose&lt;/a&gt; in 1955. Earlier, &lt;a href=\"Erik%20Ivar%20Fredholm\"&gt;Erik Ivar Fredholm&lt;/a&gt; had introduced the concept of a pseudoinverse of &lt;a href=\"integral%20operator\"&gt;integral operator&lt;/a&gt;s in 1903. When referring to a matrix, the term &lt;a href=\"pseudoinverse\"&gt;pseudoinverse&lt;/a&gt;, without further specification, is often used to indicate the Moore\u2013Penrose inverse. The term &lt;a href=\"generalized%20inverse\"&gt;generalized inverse&lt;/a&gt; is sometimes used as a synonym for pseudoinverse.\nA common use of the pseudoinverse is to compute a \"best fit\" (&lt;a href=\"Ordinary%20least%20squares\"&gt;least squares&lt;/a&gt;) solution to a &lt;a href=\"system%20of%20linear%20equations\"&gt;system of linear equations&lt;/a&gt; that lacks a solution (see below under &lt;a href=\"%23Applications\"&gt;\u00a7 Applications&lt;/a&gt;).\nAnother use is to find the minimum (&lt;a href=\"Euclidean%20norm\"&gt;Euclidean&lt;/a&gt;) norm solution to a system of linear equations with multiple solutions. The pseudoinverse facilitates the statement and proof of results in linear algebra.\nThe pseudoinverse is defined and unique for all matrices whose entries are &lt;a href=\"Real%20number\"&gt;real&lt;/a&gt; or &lt;a href=\"Complex%20number\"&gt;complex&lt;/a&gt; numbers. It can be computed using the &lt;a href=\"singular%20value%20decomposition\"&gt;singular value decomposition&lt;/a&gt;.\nNotation.\nIn the following discussion, the following conventions are adopted.\nDefinition.\nFor , a pseudoinverse of is defined as a matrix satisfying all of the following four criteria, known as the Moore\u2013Penrose conditions:\n exists for any matrix , but, when the latter has full &lt;a href=\"rank%20%28linear%20algebra%29\"&gt;rank&lt;/a&gt; (that is, the rank of is ), then can be expressed as a simple algebraic formula.\nIn particular, when has linearly independent columns (and thus matrix is invertible), can be computed as\nformula_3\nThis particular pseudoinverse constitutes a \"left inverse\", since, in this case, formula_4.\nWhen has linearly independent rows (matrix is invertible), can be computed as\nformula_5\nThis is a \"right inverse\", as formula_6.\nProperties.\nExistence and uniqueness.\nThe pseudoinverse exists and is unique: for any matrix , there is precisely one matrix , that satisfies the four properties of the definition.\nA matrix satisfying the first condition of the definition is known as a generalized inverse. If the matrix also satisfies the second definition, it is called a &lt;a href=\"generalized%20inverse%23Types%20of%20generalized%20inverses\"&gt;generalized \"reflexive\" inverse&lt;/a&gt;. Generalized inverses always exist but are not in general unique. Uniqueness is a consequence of the last two conditions.\nBasic properties.\nProofs for the properties below can be found in the &lt;a href=\"Proofs%20involving%20the%20Moore%E2%80%93Penrose%20inverse\"&gt;proofs subpage&lt;/a&gt;.\nIdentities.\nThe following identity formula can be used to cancel or expand certain subexpressions involving pseudoinverses:\nformula_13\nEquivalently, substituting formula_14 for formula_15 gives\nformula_16\nwhile substituting formula_17 for formula_15 gives\nformula_19\nReduction to Hermitian case.\nThe computation of the pseudoinverse is reducible to its construction in the Hermitian case. This is possible through the equivalences:\nformula_20\nformula_21\nas and are Hermitian.\nProducts.\nSuppose . Then the following are equivalent:\nThe following are sufficient conditions for :\nThe following is a necessary condition for :\nThe last sufficient condition yields the equalities\nformula_33\nNB: The equality does not hold in general.\nSee the counterexample:\nformula_34\nProjectors.\nformula_35 and formula_36 are &lt;a href=\"projection%20%28linear%20algebra%29\"&gt;orthogonal projection operators&lt;/a&gt;, that is, they are Hermitian (formula_37, formula_38) and idempotent (formula_39 and formula_40). The following hold:\nThe last two properties imply the following identities:\nAnother property is the following: if is Hermitian and idempotent (true if and only if it represents an orthogonal projection), then, for any matrix the following equation holds:\nformula_47\nThis can be proven by defining matrices formula_48, formula_49, and checking that is indeed a pseudoinverse for by verifying that the defining properties of the pseudoinverse hold, when is Hermitian and idempotent.\nFrom the last property it follows that, if is Hermitian and idempotent, for any matrix \nformula_50\nFinally, if is an orthogonal projection matrix, then its pseudoinverse trivially coincides with the matrix itself, that is, formula_51.\nGeometric construction.\nIf we view the matrix as a linear map over the field then can be decomposed as follows. We write for the &lt;a href=\"direct%20sum%20of%20modules\"&gt;direct sum&lt;/a&gt;, for the &lt;a href=\"orthogonal%20complement\"&gt;orthogonal complement&lt;/a&gt;, for the &lt;a href=\"kernel%20%28linear%20algebra%29\"&gt;kernel&lt;/a&gt; of a map, and for the &lt;a href=\"image%20%28mathematics%29\"&gt;image&lt;/a&gt; of a map. Notice that formula_52 and formula_53. The restriction formula_54 is then an isomorphism. This implies that on is the inverse of this isomorphism, and is zero on formula_55\nIn other words: To find for given in , first project orthogonally onto the range of , finding a point in the range. Then form , that is, find those vectors in that sends to . This will be an affine subspace of parallel to the kernel of . The element of this subspace that has the smallest length (that is, is closest to the origin) is the answer we are looking for. It can be found by taking an arbitrary member of and projecting it orthogonally onto the orthogonal complement of the kernel of .\nThis description is closely related to the &lt;a href=\"Moore%E2%80%93Penrose%20inverse%23Minimum%20norm%20solution%20to%20a%20linear%20system\"&gt;Minimum norm solution to a linear system&lt;/a&gt;.\nSubspaces.\nformula_56\nLimit relations.\nThe pseudoinverse are limits:\nformula_57\n(see &lt;a href=\"Tikhonov%20regularization\"&gt;Tikhonov regularization&lt;/a&gt;). These limits exist even if or do not exist.\nContinuity.\nIn contrast to ordinary matrix inversion, the process of taking pseudoinverses is not &lt;a href=\"continuous%20function\"&gt;continuous&lt;/a&gt;: if the sequence converges to the matrix (in the &lt;a href=\"matrix%20norm\"&gt;maximum norm or Frobenius norm&lt;/a&gt;, say), then need not converge to . However, if all the matrices have the same rank as , will converge to .\nDerivative.\nThe derivative of a real valued pseudoinverse matrix which has constant rank at a point may be calculated in terms of the derivative of the original matrix:\nformula_58\nExamples.\nSince for invertible matrices the pseudoinverse equals the usual inverse, only examples of non-invertible matrices are considered below.\nSpecial cases.\nScalars.\nIt is also possible to define a pseudoinverse for scalars and vectors. This amounts to treating these as matrices. The pseudoinverse of a scalar is zero if is zero and the reciprocal of otherwise:\nformula_59\nVectors.\nThe pseudoinverse of the null (all zero) vector is the transposed null vector. The pseudoinverse of a non-null vector is the conjugate transposed vector divided by its squared magnitude:\nformula_60\nLinearly independent columns.\nIf the columns of are &lt;a href=\"linear%20independence\"&gt;linearly independent&lt;/a&gt;\n(so that ), then is invertible. In this case, an explicit formula is:\nformula_61.\nIt follows that is then a left inverse of : \u00a0 formula_62.\nLinearly independent rows.\nIf the rows of are linearly independent (so that ), then is invertible. In this case, an explicit formula is:\nformula_63.\nIt follows that is a right inverse of : \u00a0 formula_64.\nOrthonormal columns or rows.\nThis is a special case of either full column rank or full row rank (treated above). If has orthonormal columns (formula_65) or orthonormal rows (formula_66), then:\nformula_67\nNormal matrices.\nIf is a &lt;a href=\"Normal%20matrix\"&gt;Normal matrix&lt;/a&gt;; that is, it commutes with its conjugate transpose; then its pseudoinverse can be computed by diagonalizing it, mapping all nonzero eigenvalues to their inverses, and mapping zero eigenvalues to zero. A corollary is that commuting with its transpose implies that it commutes with its pseudoinverse.\nOrthogonal projection matrices.\nThis is a special case of a Normal matrix with eigenvalues 0 and 1. If is an orthogonal projection matrix, that is, formula_68 and formula_69, then the pseudoinverse trivially coincides with the matrix itself:\nformula_70\nCirculant matrices.\nFor a &lt;a href=\"circulant%20matrix\"&gt;circulant matrix&lt;/a&gt; , the singular value decomposition is given by the &lt;a href=\"Fourier%20transform\"&gt;Fourier transform&lt;/a&gt;, that is, the singular values are the Fourier coefficients. Let be the &lt;a href=\"DFT%20matrix\"&gt;Discrete Fourier Transform (DFT) matrix&lt;/a&gt;, then\nformula_71\nConstruction.\nRank decomposition.\nLet denote the &lt;a href=\"rank%20%28matrix%20theory%29\"&gt;rank&lt;/a&gt; of . Then can be &lt;a href=\"rank%20factorization\"&gt;(rank) decomposed&lt;/a&gt; as\nformula_72 where and are of rank . Then formula_73.\nThe QR method.\nFor formula_74 computing the product or and their inverses explicitly is often a source of numerical rounding errors and computational cost in practice. An alternative approach using the &lt;a href=\"QR%20decomposition\"&gt;QR decomposition&lt;/a&gt; of may be used instead.\nConsider the case when is of full column rank, so that formula_75. Then the &lt;a href=\"Cholesky%20decomposition\"&gt;Cholesky decomposition&lt;/a&gt; formula_76, where is an &lt;a href=\"upper%20triangular%20matrix\"&gt;upper triangular matrix&lt;/a&gt;, may be used. Multiplication by the inverse is then done easily by solving a system with multiple right-hand sides,\nformula_77\nwhich may be solved by &lt;a href=\"forward%20substitution\"&gt;forward substitution&lt;/a&gt; followed by &lt;a href=\"back%20substitution\"&gt;back substitution&lt;/a&gt;.\nThe Cholesky decomposition may be computed without forming explicitly, by alternatively using the &lt;a href=\"QR%20decomposition\"&gt;QR decomposition&lt;/a&gt; of formula_78, where formula_79 has orthonormal columns, formula_80, and is upper triangular. Then\nformula_81\nso is the Cholesky factor of .\nThe case of full row rank is treated similarly by using the formula formula_82 and using a similar argument, swapping the roles of and .\nSingular value decomposition (SVD).\nA computationally simple and accurate way to compute the pseudoinverse is by using the &lt;a href=\"singular%20value%20decomposition\"&gt;singular value decomposition&lt;/a&gt;. If formula_83 is the singular value decomposition of , then formula_84. For a &lt;a href=\"rectangular%20diagonal%20matrix\"&gt;rectangular diagonal matrix&lt;/a&gt; such as , we get the pseudoinverse by taking the reciprocal of each non-zero element on the diagonal, leaving the zeros in place, and then transposing the matrix. In numerical computation, only elements larger than some small tolerance are taken to be nonzero, and the others are replaced by zeros. For example, in the &lt;a href=\"MATLAB\"&gt;MATLAB&lt;/a&gt; or &lt;a href=\"GNU%20Octave\"&gt;GNU Octave&lt;/a&gt; function , the tolerance is taken to be , where \u03b5 is the &lt;a href=\"machine%20epsilon\"&gt;machine epsilon&lt;/a&gt;.\nThe computational cost of this method is dominated by the cost of computing the SVD, which is several times higher than matrix\u2013matrix multiplication, even if a state-of-the art implementation (such as that of &lt;a href=\"LAPACK\"&gt;LAPACK&lt;/a&gt;) is used.\nThe above procedure shows why taking the pseudoinverse is not a continuous operation: if the original matrix has a singular value 0 (a diagonal entry of the matrix above), then modifying slightly may turn this zero into a tiny positive number, thereby affecting the pseudoinverse dramatically as we now have to take the reciprocal of a tiny number.\nBlock matrices.\n&lt;a href=\"Block%20matrix%20pseudoinverse\"&gt;Optimized approaches&lt;/a&gt; exist for calculating the pseudoinverse of block structured matrices.\nThe iterative method of Ben-Israel and Cohen.\nAnother method for computing the pseudoinverse (cf. &lt;a href=\"Drazin%20inverse\"&gt;Drazin inverse&lt;/a&gt;) uses the recursion\nformula_85\nwhich is sometimes referred to as hyper-power sequence. This recursion produces a sequence converging quadratically to the pseudoinverse of if it is started with an appropriate satisfying formula_86. The choice formula_87 (where formula_88, with denoting the largest singular value of ) has been argued not to be competitive to the method using the SVD mentioned above, because even for moderately ill-conditioned matrices it takes a long time before enters the region of quadratic convergence. However, if started with already close to the Moore\u2013Penrose inverse and formula_86, for example formula_90, convergence is fast (quadratic).\nUpdating the pseudoinverse.\nFor the cases where has full row or column rank, and the inverse of the correlation matrix ( for with full row rank or for full column rank) is already known, the pseudoinverse for matrices related to can be computed by applying the &lt;a href=\"Sherman%E2%80%93Morrison%E2%80%93Woodbury%20formula\"&gt;Sherman\u2013Morrison\u2013Woodbury formula&lt;/a&gt; to update the inverse of the correlation matrix, which may need less work. In particular, if the related matrix differs from the original one by only a changed, added or deleted row or column, incremental algorithms exist that exploit the relationship.\nSimilarly, it is possible to update the Cholesky factor when a row or column is added, without creating the inverse of the correlation matrix explicitly. However, updating the pseudoinverse in the general rank-deficient case is much more complicated.\nSoftware libraries.\nHigh-quality implementations of SVD, QR, and back substitution are available in &lt;a href=\"Singular%20value%20decomposition%23Implementations\"&gt;standard libraries&lt;/a&gt;, such as &lt;a href=\"LAPACK\"&gt;LAPACK&lt;/a&gt;. Writing one's own implementation of SVD is a major programming project that requires a significant &lt;a href=\"Floating%20point%23Accuracy%20problems\"&gt;numerical expertise&lt;/a&gt;. In special circumstances, such as &lt;a href=\"parallel%20computing\"&gt;parallel computing&lt;/a&gt; or &lt;a href=\"embedded%20computing\"&gt;embedded computing&lt;/a&gt;, however, alternative implementations by QR or even the use of an explicit inverse might be preferable, and custom implementations may be unavoidable.\nThe Python package &lt;a href=\"NumPy\"&gt;NumPy&lt;/a&gt; provides a pseudoinverse calculation through its functions codice_1 and codice_2; its codice_3 uses the SVD-based algorithm. &lt;a href=\"SciPy\"&gt;SciPy&lt;/a&gt; adds a function codice_4 that uses a least-squares solver. \nThe MASS package for &lt;a href=\"R%20%28programming%20language%29\"&gt;R&lt;/a&gt; provides a calculation of the Moore\u2013Penrose inverse through the codice_5 function. The codice_5 function calculates a pseudoinverse using the singular value decomposition provided by the codice_7 function in the base R package. An alternative is to employ the codice_3 function available in the pracma package.\nThe &lt;a href=\"GNU%20Octave\"&gt;Octave programming language&lt;/a&gt; provides a pseudoinverse through the standard package function codice_3 and the codice_10 method.\nIn &lt;a href=\"Julia%20%28programming%20language%29\"&gt;Julia (programming language)&lt;/a&gt;, the LinearAlgebra package of the standard library provides an implementation of the Moore-Penrose inverse codice_11 implemented via singular-value decomposition.\nApplications.\nLinear least-squares.\nThe pseudoinverse provides a &lt;a href=\"linear%20least%20squares%20%28mathematics%29\"&gt;least squares&lt;/a&gt; solution to a &lt;a href=\"system%20of%20linear%20equations\"&gt;system of linear equations&lt;/a&gt;.\nFor , given a system of linear equations\nformula_91\nin general, a vector that solves the system may not exist, or if one does exist, it may not be unique. The pseudoinverse solves the \"least-squares\" problem as follows:\nThis result is easily extended to systems with multiple right-hand sides, when the Euclidean norm is replaced by the Frobenius norm. Let .\nObtaining all solutions of a linear system.\nIf the linear system\nformula_99\nhas any solutions, they are all given by\nformula_100\nfor arbitrary vector . Solution(s) exist if and only if formula_101. If the latter holds, then the solution is unique if and only if has full column rank, in which case is a zero matrix. If solutions exist but does not have full column rank, then we have an &lt;a href=\"indeterminate%20system\"&gt;indeterminate system&lt;/a&gt;, all of whose infinitude of solutions are given by this last equation.\nMinimum norm solution to a linear system.\nFor linear systems formula_102 with non-unique solutions (such as under-determined systems), the pseudoinverse may be used to construct the solution of minimum &lt;a href=\"Euclidean%20norm\"&gt;Euclidean norm&lt;/a&gt;\nformula_103 among all solutions.\nThis result is easily extended to systems with multiple right-hand sides, when the Euclidean norm is replaced by the Frobenius norm. Let .\nCondition number.\nUsing the pseudoinverse and a &lt;a href=\"matrix%20norm\"&gt;matrix norm&lt;/a&gt;, one can define a &lt;a href=\"condition%20number\"&gt;condition number&lt;/a&gt; for any matrix:\nformula_110\nA large condition number implies that the problem of finding least-squares solutions to the corresponding system of linear equations is ill-conditioned in the sense that small errors in the entries of can lead to huge errors in the entries of the solution.\nGeneralizations.\nBesides for matrices over real and complex numbers, the conditions hold for matrices over &lt;a href=\"biquaternion\"&gt;biquaternion&lt;/a&gt;s, also called \"complex quaternions\".\nIn order to solve more general least-squares problems, one can define Moore\u2013Penrose inverses for all continuous linear operators between two &lt;a href=\"Hilbert%20space\"&gt;Hilbert space&lt;/a&gt;s and , using the same four conditions as in our definition above. It turns out that not every continuous linear operator has a continuous linear pseudoinverse in this sense. Those that do are precisely the ones whose range is &lt;a href=\"closed%20set\"&gt;closed&lt;/a&gt; in .\nA notion of pseudoinverse exists for matrices over an arbitrary &lt;a href=\"Field%20%28mathematics%29\"&gt;field&lt;/a&gt; equipped with an arbitrary &lt;a href=\"Involution%20%28mathematics%29\"&gt;involutive&lt;/a&gt; &lt;a href=\"automorphism\"&gt;automorphism&lt;/a&gt;. In this more general setting, a given matrix doesn't always have a pseudoinverse. The necessary and sufficient condition for a pseudoinverse to exist is that formula_111 where formula_17 denotes the result of applying the involution operation to the transpose of formula_15. When it does exist, it is unique. Example: Consider the field of complex numbers equipped with the &lt;a href=\"Identity%20function\"&gt;identity involution&lt;/a&gt; (as opposed to the involution considered elsewhere in the article); do there exist matrices that fail to have pseudoinverses in this sense? Consider the matrix formula_114. Observe that formula_115 while formula_116. So this matrix doesn't have a pseudoinverse in this sense.\nIn &lt;a href=\"abstract%20algebra\"&gt;abstract algebra&lt;/a&gt;, a Moore\u2013Penrose inverse may be defined on a &lt;a href=\"%2A-regular%20semigroup\"&gt;*-regular semigroup&lt;/a&gt;. This abstract definition coincides with the one in linear algebra."
}