{
    "id": "672731",
    "revid": "1061383759",
    "url": "https://en.wikipedia.org/wiki?curid=672731",
    "title": "Matrix exponential",
    "text": "In &lt;a href=\"mathematics\"&gt;mathematics&lt;/a&gt;, the matrix exponential is a &lt;a href=\"matrix%20function\"&gt;matrix function&lt;/a&gt; on &lt;a href=\"square%20matrix\"&gt;square matrices&lt;/a&gt; analogous to the ordinary &lt;a href=\"exponential%20function\"&gt;exponential function&lt;/a&gt;. It is used to solve systems of linear differential equations. In the theory of Lie groups, the matrix exponential gives the connection between a matrix &lt;a href=\"Lie%20algebra\"&gt;Lie algebra&lt;/a&gt; and the corresponding &lt;a href=\"Lie%20group\"&gt;Lie group&lt;/a&gt;.\nLet be an &lt;a href=\"real%20number\"&gt;real&lt;/a&gt; or &lt;a href=\"complex%20number\"&gt;complex&lt;/a&gt; &lt;a href=\"matrix%20%28mathematics%29\"&gt;matrix&lt;/a&gt;. The exponential of , denoted by or , is the matrix given by the &lt;a href=\"power%20series\"&gt;power series&lt;/a&gt;\nformula_1\nwhere formula_2 is defined to be the identity matrix formula_3 with the same dimensions as formula_4.\nThe above series always converges, so the exponential of is well-defined. If is a 1\u00d71 matrix the matrix exponential of is a 1\u00d71 matrix whose single element is the ordinary &lt;a href=\"Exponential%20function\"&gt;exponential&lt;/a&gt; of the single element of .\nProperties.\nElementary properties.\nLet and be complex matrices and let and be arbitrary complex numbers. We denote the &lt;a href=\"identity%20matrix\"&gt;identity matrix&lt;/a&gt; by and the &lt;a href=\"zero%20matrix\"&gt;zero matrix&lt;/a&gt; by 0. The matrix exponential satisfies the following properties.\nWe begin with the properties that are immediate consequences of the definition as a power series:\nThe next key result is this one:\nThe proof of this identity is the same as the standard power-series argument for the corresponding identity for the exponential of real numbers. That is to say, \"as long as formula_4 and formula_8 commute\", it makes no difference to the argument whether formula_4 and formula_8 are numbers or matrices. It is important to note that this identity typically does not hold if formula_4 and formula_8 do not commute (see &lt;a href=\"%23Inequalities%20for%20exponentials%20of%20Hermitian%20matrices\"&gt;Golden-Thompson inequality&lt;/a&gt; below).\nConsequences of the preceding identity are the following:\nUsing the above results, we can easily verify the following claims. If is &lt;a href=\"symmetric%20matrix\"&gt;symmetric&lt;/a&gt; then is also symmetric, and if is &lt;a href=\"skew-symmetric%20matrix\"&gt;skew-symmetric&lt;/a&gt; then is &lt;a href=\"orthogonal%20matrix\"&gt;orthogonal&lt;/a&gt;. If is &lt;a href=\"Hermitian%20matrix\"&gt;Hermitian&lt;/a&gt; then is also Hermitian, and if is &lt;a href=\"skew-Hermitian%20matrix\"&gt;skew-Hermitian&lt;/a&gt; then is &lt;a href=\"unitary%20matrix\"&gt;unitary&lt;/a&gt;.\nFinally, a &lt;a href=\"Laplace%20transform\"&gt;Laplace transform&lt;/a&gt; of matrix exponentials amounts to the &lt;a href=\"resolvent%20formalism\"&gt;resolvent&lt;/a&gt;,\nformula_13\nfor all sufficiently large positive values of .\nLinear differential equation systems.\nOne of the reasons for the importance of the matrix exponential is that it can be used to solve systems of linear &lt;a href=\"ordinary%20differential%20equations\"&gt;ordinary differential equations&lt;/a&gt;. The solution of\nformula_14\nwhere is a constant matrix, is given by\nformula_15\nThe matrix exponential can also be used to solve the inhomogeneous equation\nformula_16\nSee the section on &lt;a href=\"%23Applications\"&gt;applications&lt;/a&gt; below for examples.\nThere is no closed-form solution for differential equations of the form\nformula_17\nwhere is not constant, but the &lt;a href=\"Magnus%20series\"&gt;Magnus series&lt;/a&gt; gives the solution as an infinite sum.\nThe determinant of the matrix exponential.\nBy &lt;a href=\"Jacobi%27s%20formula\"&gt;Jacobi's formula&lt;/a&gt;, for any complex square matrix the following &lt;a href=\"trace%20identity\"&gt;trace identity&lt;/a&gt; holds:\nIn addition to providing a computational tool, this formula demonstrates that a matrix exponential is always an &lt;a href=\"invertible%20matrix\"&gt;invertible matrix&lt;/a&gt;. This follows from the fact that the right hand side of the above equation is always non-zero, and so , which implies that must be invertible.\nIn the real-valued case, the formula also exhibits the map\nformula_18\nto not be &lt;a href=\"surjective%20function\"&gt;surjective&lt;/a&gt;, in contrast to the complex case mentioned earlier. This follows from the fact that, for real-valued matrices, the right-hand side of the formula is always positive, while there exist invertible matrices with a negative determinant.\nReal symmetric matrices.\nThe matrix exponential of a real symmetric matrix is positive definite. Let formula_19 be an real symmetric matrix and formula_20 a column vector. Using the elementary properties of the matrix exponential and of symmetric matrices, we have:\nformula_21\nSince formula_22 is invertible, the equality only holds for formula_23, and we have formula_24 for all non-zero formula_25. Hence formula_26 is positive definite.\nThe exponential of sums.\nFor any real numbers (scalars) and we know that the exponential function satisfies . The same is true for commuting matrices. If matrices and commute (meaning that ), then,\nformula_27\nHowever, for matrices that do not commute the above equality does not necessarily hold.\nThe Lie product formula.\nEven if and do not commute, the exponential can be computed by the &lt;a href=\"Lie%20product%20formula\"&gt;Lie product formula&lt;/a&gt;\nformula_28\nUsing a large finite to approximate the above is basis of the Suzuki-Trotter expansion, often used in &lt;a href=\"Time-evolving%20block%20decimation%23The_Suzuki%25E2%2580%2593Trotter%20expansion\"&gt;numerical time evolution&lt;/a&gt;.\nThe Baker\u2013Campbell\u2013Hausdorff formula.\nIn the other direction, if and are sufficiently small (but not necessarily commuting) matrices, we have\nformula_29\nwhere may be computed as a series in &lt;a href=\"commutator\"&gt;commutator&lt;/a&gt;s of and by means of the &lt;a href=\"Baker%E2%80%93Campbell%E2%80%93Hausdorff%20formula\"&gt;Baker\u2013Campbell\u2013Hausdorff formula&lt;/a&gt;:\nformula_30\nwhere the remaining terms are all iterated commutators involving and . If and commute, then all the commutators are zero and we have simply .\nInequalities for exponentials of Hermitian matrices.\nFor &lt;a href=\"Hermitian%20matrix\"&gt;Hermitian matrices&lt;/a&gt; there is a notable theorem related to the &lt;a href=\"Matrix%20trace\"&gt;trace&lt;/a&gt; of matrix exponentials.\nIf and are Hermitian matrices, then\nformula_31\nThere is no requirement of commutativity. There are counterexamples to show that the Golden\u2013Thompson inequality cannot be extended to three matrices \u2013 and, in any event, is not guaranteed to be real for Hermitian , , . However, &lt;a href=\"Elliott%20H.%20Lieb\"&gt;Lieb&lt;/a&gt; proved that it can be generalized to three matrices if we modify the expression as follows\nformula_32\nThe exponential map.\nThe exponential of a matrix is always an &lt;a href=\"invertible%20matrix\"&gt;invertible matrix&lt;/a&gt;. The inverse matrix of is given by . This is analogous to the fact that the exponential of a complex number is always nonzero. The matrix exponential then gives us a map\nformula_33\nfrom the space of all \"n\"\u00d7\"n\" matrices to the &lt;a href=\"general%20linear%20group\"&gt;general linear group&lt;/a&gt; of degree , i.e. the &lt;a href=\"group%20%28mathematics%29\"&gt;group&lt;/a&gt; of all \"n\"\u00d7\"n\" invertible matrices. In fact, this map is &lt;a href=\"surjective\"&gt;surjective&lt;/a&gt; which means that every invertible matrix can be written as the exponential of some other matrix (for this, it is essential to consider the field C of complex numbers and not R).\nFor any two matrices and ,\nformula_34\nwhere denotes an arbitrary &lt;a href=\"matrix%20norm\"&gt;matrix norm&lt;/a&gt;. It follows that the exponential map is &lt;a href=\"continuity%20%28mathematics%29\"&gt;continuous&lt;/a&gt; and &lt;a href=\"Lipschitz%20continuous\"&gt;Lipschitz continuous&lt;/a&gt; on &lt;a href=\"compact%20set\"&gt;compact&lt;/a&gt; subsets of .\nThe map\nformula_35\ndefines a &lt;a href=\"Smooth%20function%23Smoothness\"&gt;smooth&lt;/a&gt; curve in the general linear group which passes through the identity element at .\nIn fact, this gives a &lt;a href=\"one-parameter%20subgroup\"&gt;one-parameter subgroup&lt;/a&gt; of the general linear group since\nformula_36\nThe derivative of this curve (or &lt;a href=\"tangent%20vector\"&gt;tangent vector&lt;/a&gt;) at a point \"t\" is given by\nThe derivative at is just the matrix \"X\", which is to say that \"X\" generates this one-parameter subgroup.\nMore generally, for a generic -dependent exponent, ,\nTaking the above expression outside the integral sign and expanding the integrand with the help of the &lt;a href=\"Baker%E2%80%93Campbell%E2%80%93Hausdorff%20formula\"&gt;Hadamard lemma&lt;/a&gt; one can obtain the following useful expression for the derivative of the matrix exponent,\nformula_37\nThe coefficients in the expression above are different from what appears in the exponential. For a closed form, see &lt;a href=\"derivative%20of%20the%20exponential%20map\"&gt;derivative of the exponential map&lt;/a&gt;.\nComputing the matrix exponential.\nFinding reliable and accurate methods to compute the matrix exponential is difficult, and this is still a topic of considerable current research in mathematics and numerical analysis. &lt;a href=\"Matlab\"&gt;Matlab&lt;/a&gt;, &lt;a href=\"GNU%20Octave\"&gt;GNU Octave&lt;/a&gt;, and &lt;a href=\"SciPy\"&gt;SciPy&lt;/a&gt; all use the &lt;a href=\"Pad%C3%A9%20approximant\"&gt;Pad\u00e9 approximant&lt;/a&gt;. In this section, we discuss methods that are applicable in principle to any matrix, and which can be carried out explicitly for small matrices. Subsequent sections describe methods suitable for numerical evaluation on large matrices.\nDiagonalizable case.\nIf a matrix is &lt;a href=\"diagonal%20matrix\"&gt;diagonal&lt;/a&gt;:\nformula_38\nthen its exponential can be obtained by exponentiating each entry on the main diagonal:\nformula_39\nThis result also allows one to exponentiate &lt;a href=\"diagonalizable%20matrix\"&gt;diagonalizable matrices&lt;/a&gt;. If\nand is diagonal, then\nApplication of &lt;a href=\"Sylvester%27s%20formula\"&gt;Sylvester's formula&lt;/a&gt; yields the same result. (To see this, note that addition and multiplication, hence also exponentiation, of diagonal matrices is equivalent to element-wise addition and multiplication, and hence exponentiation; in particular, the \"one-dimensional\" exponentiation is felt element-wise for the diagonal case.)\nExample : Diagonalizable.\nFor example, the matrix\nformula_40\ncan be diagonalized as\nformula_41\nThus,\nformula_42\nNilpotent case.\nA matrix is &lt;a href=\"nilpotent%20matrix\"&gt;nilpotent&lt;/a&gt; if for some integer \"q\". In this case, the matrix exponential can be computed directly from the series expansion, as the series terminates after a finite number of terms:\nformula_43\nSince the series has a finite number of steps, it is a matrix polynomial, which can be &lt;a href=\"Polynomial%20evaluation%23Matrix%20polynomials\"&gt;computed efficiently&lt;/a&gt;.\nGeneral case.\nUsing the Jordan\u2013Chevalley decomposition.\nBy the &lt;a href=\"Jordan%E2%80%93Chevalley%20decomposition\"&gt;Jordan\u2013Chevalley decomposition&lt;/a&gt;, any formula_44 matrix \"X\" with complex entries can be expressed as\nformula_45\nwhere\nThis means that we can compute the exponential of \"X\" by reducing to the previous two cases:\nformula_46\nNote that we need the commutativity of \"A\" and \"N\" for the last step to work.\nUsing the Jordan canonical form.\nA closely related method is, if the field is &lt;a href=\"algebraically%20closed\"&gt;algebraically closed&lt;/a&gt;, to work with the &lt;a href=\"Jordan%20form\"&gt;Jordan form&lt;/a&gt; of . Suppose that where is the Jordan form of . Then\nformula_47\nAlso, since\nformula_48\nTherefore, we need only know how to compute the matrix exponential of a Jordan block. But each Jordan block is of the form\nformula_49\nwhere is a special nilpotent matrix. The matrix exponential of is then given by\nformula_50\nProjection case.\nIf is a &lt;a href=\"projection%20matrix\"&gt;projection matrix&lt;/a&gt; (i.e. is &lt;a href=\"idempotent\"&gt;idempotent&lt;/a&gt;: ), its matrix exponential is:\nDeriving this by expansion of the exponential function, each power of reduces to which becomes a common factor of the sum:\nformula_51\nRotation case.\nFor a simple rotation in which the perpendicular unit vectors and specify a plane, the &lt;a href=\"Rotation%20matrix%23Exponential%20map\"&gt;rotation matrix&lt;/a&gt; can be expressed in terms of a similar exponential function involving a &lt;a href=\"Euler%27s%20rotation%20theorem%23Generators%20of%20rotations\"&gt;generator&lt;/a&gt; and angle .\nformula_52\nformula_53\nThe formula for the exponential results from reducing the powers of in the series expansion and identifying the respective series coefficients of and with and respectively. The second expression here for is the same as the expression for in the article containing the derivation of the &lt;a href=\"Euler%27s%20rotation%20theorem%23Generators%20of%20rotations\"&gt;generator&lt;/a&gt;, .\nIn two dimensions, if formula_54 and formula_55, then formula_56, formula_57, and\nformula_58\nreduces to the standard matrix for a plane rotation.\nThe matrix &lt;a href=\"Projection%20%28linear%20algebra%29\"&gt;projects&lt;/a&gt; a vector onto the -plane and the rotation only affects this part of the vector. An example illustrating this is a rotation of in the plane spanned by and ,\nformula_59\nformula_60\nLet , so and its products with and are zero. This will allow us to evaluate powers of .\nformula_61\nEvaluation by Laurent series.\nBy virtue of the &lt;a href=\"Cayley%E2%80%93Hamilton%20theorem\"&gt;Cayley\u2013Hamilton theorem&lt;/a&gt; the matrix exponential is expressible as a polynomial of order \u22121.\nIf and are nonzero polynomials in one variable, such that , and if the &lt;a href=\"meromorphic%20function\"&gt;meromorphic function&lt;/a&gt;\nformula_62\nis &lt;a href=\"entire%20function\"&gt;entire&lt;/a&gt;, then\nformula_63\nTo prove this, multiply the first of the two above equalities by and replace by .\nSuch a polynomial can be found as follows\u2212see &lt;a href=\"Sylvester%27s%20formula\"&gt;Sylvester's formula&lt;/a&gt;. Letting be a root of , is solved from the product of by the &lt;a href=\"Laurent%20series%23Principal%20part\"&gt;principal part&lt;/a&gt; of the &lt;a href=\"Laurent%20series\"&gt;Laurent series&lt;/a&gt; of at : It is proportional to the relevant &lt;a href=\"Frobenius%20covariant\"&gt;Frobenius covariant&lt;/a&gt;. Then the sum \"St\" of the \"Qa,t\", where runs over all the roots of , can be taken as a particular . All the other \"Qt\" will be obtained by adding a multiple of to . In particular, , the &lt;a href=\"Sylvester%27s%20formula\"&gt;Lagrange-Sylvester polynomial&lt;/a&gt;, is the only whose degree is less than that of .\nExample: Consider the case of an arbitrary 2\u00d72 matrix,\nformula_64\nThe exponential matrix , by virtue of the &lt;a href=\"Cayley%E2%80%93Hamilton%20theorem\"&gt;Cayley\u2013Hamilton theorem&lt;/a&gt;, must be of the form\nformula_65\nLet and be the roots of the &lt;a href=\"characteristic%20polynomial\"&gt;characteristic polynomial&lt;/a&gt; of ,\nformula_66\nThen we have\nformula_67\nhence\nformula_68\nif ; while, if ,\nformula_69\nso that\nformula_70\nDefining\nformula_71\nwe have\nformula_72\nwhere is 0 if , and if .\nThus,\nThus, as indicated above, the matrix having decomposed into the sum of two mutually commuting pieces, the traceful piece and the traceless piece,\nformula_73\nthe matrix exponential reduces to a plain product of the exponentials of the two respective pieces. This is a formula often used in physics, as it amounts to the analog of &lt;a href=\"Euler%27s%20formula\"&gt;Euler's formula&lt;/a&gt; for &lt;a href=\"Pauli%20spin%20matrices%23Exponential%20of%20a%20Pauli%20vector\"&gt;Pauli spin matrices&lt;/a&gt;, that is rotations of the doublet representation of the group &lt;a href=\"SU%282%29\"&gt;SU(2)&lt;/a&gt;.\nThe polynomial can also be given the following \"&lt;a href=\"interpolation\"&gt;interpolation&lt;/a&gt;\" characterization. Define , and . Then is the unique degree polynomial which satisfies whenever is less than the multiplicity of as a root of . We assume, as we obviously can, that is the &lt;a href=\"Minimal%20polynomial%20%28linear%20algebra%29\"&gt;minimal polynomial&lt;/a&gt; of . We further assume that is a &lt;a href=\"diagonalizable%20matrix\"&gt;diagonalizable matrix&lt;/a&gt;. In particular, the roots of are simple, and the \"&lt;a href=\"interpolation\"&gt;interpolation&lt;/a&gt;\" characterization indicates that is given by the &lt;a href=\"Lagrange%20interpolation\"&gt;Lagrange interpolation&lt;/a&gt; formula, so it is the &lt;a href=\"Sylvester%27s%20formula\"&gt;Lagrange\u2212Sylvester polynomial&lt;/a&gt; .\nAt the other extreme, if , then\nformula_74\nThe simplest case not covered by the above observations is when formula_75 with , which yields\nformula_76\nEvaluation by implementation of &lt;a href=\"Sylvester%27s%20formula\"&gt;Sylvester's formula&lt;/a&gt;.\nA practical, expedited computation of the above reduces to the following rapid steps. Recall from above that an \"n\u00d7n\" matrix amounts to a linear combination of the first \u22121 powers of by the &lt;a href=\"Cayley%E2%80%93Hamilton%20theorem\"&gt;Cayley\u2013Hamilton theorem&lt;/a&gt;. For &lt;a href=\"diagonalizable%20matrix\"&gt;diagonalizable&lt;/a&gt; matrices, as illustrated above, e.g. in the 2\u00d72 case, &lt;a href=\"Sylvester%27s%20formula\"&gt;Sylvester's formula&lt;/a&gt; yields , where the s are the &lt;a href=\"Frobenius%20covariant\"&gt;Frobenius covariant&lt;/a&gt;s of .\nIt is easiest, however, to simply solve for these s directly, by evaluating this expression and its first derivative at , in terms of and , to find the same answer as above.\nBut this simple procedure also works for &lt;a href=\"defective%20matrix\"&gt;defective&lt;/a&gt; matrices, in a generalization due to Buchheim. This is illustrated here for a 4\u00d74 example of a matrix which is \"not diagonalizable\", and the s are not projection matrices.\nConsider\nformula_77\nwith eigenvalues and , each with a multiplicity of two.\nConsider the exponential of each eigenvalue multiplied by , . Multiply each exponentiated eigenvalue by the corresponding undetermined coefficient matrix . If the eigenvalues have an algebraic multiplicity greater than 1, then repeat the process, but now multiplying by an extra factor of for each repetition, to ensure linear independence.\nSum all such terms, here four such,\nformula_79\nTo solve for all of the unknown matrices in terms of the first three powers of and the identity, one needs four equations, the above one providing one such at = 0. Further, differentiate it with respect to ,\nformula_80\nand again,\nformula_81\nand once more,\nformula_82\nSetting = 0 in these four equations, the four coefficient matrices s may now be solved for,\nformula_83\nto yield\nformula_84\nSubstituting with the value for yields the coefficient matrices\nformula_85\nso the final answer is\nformula_86\nThe procedure is much shorter than &lt;a href=\"Matrix%20differential%20equation%23Putzer%20Algorithm%20for%20computing%20eAt\"&gt;Putzer's algorithm&lt;/a&gt; sometimes utilized in such cases.\nIllustrations.\nSuppose that we want to compute the exponential of\nformula_87\nIts &lt;a href=\"Jordan%20normal%20form\"&gt;Jordan form&lt;/a&gt; is\nformula_88\nwhere the matrix \"P\" is given by\nformula_89\nLet us first calculate exp(\"J\"). We have\nformula_90\nThe exponential of a 1\u00d71 matrix is just the exponential of the one entry of the matrix, so . The exponential of \"J\"2(16) can be calculated by the formula mentioned above; this yields\nformula_91\nTherefore, the exponential of the original matrix is\nformula_92\nApplications.\nLinear differential equations.\nThe matrix exponential has applications to systems of &lt;a href=\"linear%20differential%20equation\"&gt;linear differential equation&lt;/a&gt;s. (See also &lt;a href=\"matrix%20differential%20equation\"&gt;matrix differential equation&lt;/a&gt;.) Recall from earlier in this article that a \"homogeneous\" differential equation of the form\nformula_93\nhas solution .\nIf we consider the vector\nformula_94\nwe can express a system of \"inhomogeneous\" coupled linear differential equations as\nformula_95\nMaking an &lt;a href=\"ansatz\"&gt;ansatz&lt;/a&gt; to use an integrating factor of and multiplying throughout, yields\nformula_96\nThe second step is possible due to the fact that, if , then . So, calculating leads to the solution to the system, by simply integrating the third step with respect to .\nExample (homogeneous).\nConsider the system\nformula_97\nThe associated &lt;a href=\"defective%20matrix\"&gt;defective matrix&lt;/a&gt; is\nformula_98\nThe matrix exponential is\nformula_99\nso that the general solution of the homogeneous system is\nformula_100\namounting to\nformula_101\nExample (inhomogeneous).\nConsider now the inhomogeneous system\nformula_102\nWe again have\nformula_103\nand\nformula_104\nFrom before, we already have the general solution to the homogeneous equation. Since the sum of the homogeneous and particular solutions give the general solution to the inhomogeneous problem, we now only need find the particular solution.\nWe have, by above,\nformula_105\nwhich could be further simplified to get the requisite particular solution determined through variation of parameters.\nNote c = y\"p\"(0). For more rigor, see the following generalization.\nInhomogeneous case generalization: variation of parameters.\nFor the inhomogeneous case, we can use &lt;a href=\"integrating%20factor\"&gt;integrating factor&lt;/a&gt;s (a method akin to &lt;a href=\"variation%20of%20parameters\"&gt;variation of parameters&lt;/a&gt;). We seek a particular solution of the form ,\nformula_106\nFor to be a solution,\nformula_107\nThus,\nformula_108\nwhere is determined by the initial conditions of the problem.\nMore precisely, consider the equation\nformula_109\nwith the initial condition , where\nLeft-multiplying the above displayed equality by yields\nformula_112\nWe claim that the solution to the equation\nformula_113\nwith the initial conditions formula_114 for is\nformula_115\nwhere the notation is as follows:\n is the coefficient of formula_119 in the polynomial denoted by formula_120 in Subsection &lt;a href=\"matrix%20exponential%23Evaluation%20by%20Laurent%20series\"&gt;Evaluation by Laurent series&lt;/a&gt; above.\nTo justify this claim, we transform our order scalar equation into an order one vector equation by the usual &lt;a href=\"Ordinary%20differential%20equation%23Reduction%20to%20a%20first-order%20system\"&gt;reduction to a first order system&lt;/a&gt;. Our vector equation takes the form\nformula_121\nwhere is the &lt;a href=\"transpose\"&gt;transpose&lt;/a&gt; &lt;a href=\"companion%20matrix\"&gt;companion matrix&lt;/a&gt; of . We solve this equation as explained above, computing the matrix exponentials by the observation made in Subsection &lt;a href=\"matrix%20exponential%23Evaluation%20by%20implementation%20of%20Sylvester%27s%20formula\"&gt;Evaluation by implementation of Sylvester's formula&lt;/a&gt; above.\nIn the case = 2 we get the following statement. The solution to\nformula_122\nis\nformula_123\nwhere the functions and are as in Subsection &lt;a href=\"matrix%20exponential%23Evaluation%20by%20Laurent%20series\"&gt;Evaluation by Laurent series&lt;/a&gt; above.\nMatrix-matrix exponentials.\nThe matrix exponential of another matrix (matrix-matrix exponential), is defined as\nformula_124\nformula_125\nfor any &lt;a href=\"normal%20matrix\"&gt;normal&lt;/a&gt; and &lt;a href=\"non-singular\"&gt;non-singular&lt;/a&gt; matrix , and any complex matrix .\nFor matrix-matrix exponentials, there is a distinction between the left exponential and the right exponential , because the multiplication operator for matrix-to-matrix is not &lt;a href=\"commutative\"&gt;commutative&lt;/a&gt;. Moreover,"
}