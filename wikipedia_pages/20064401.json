{
    "id": "20064401",
    "revid": "1067772610",
    "url": "https://en.wikipedia.org/wiki?curid=20064401",
    "title": "Pinsker's inequality",
    "text": "In &lt;a href=\"information%20theory\"&gt;information theory&lt;/a&gt;, Pinsker's inequality, named after its inventor &lt;a href=\"Mark%20Semenovich%20Pinsker\"&gt;Mark Semenovich Pinsker&lt;/a&gt;, is an &lt;a href=\"inequality%20%28mathematics%29\"&gt;inequality&lt;/a&gt; that bounds the &lt;a href=\"total%20variation%20distance\"&gt;total variation distance&lt;/a&gt; (or statistical distance) in terms of the &lt;a href=\"Kullback%E2%80%93Leibler%20divergence\"&gt;Kullback\u2013Leibler divergence&lt;/a&gt;.\nThe inequality is tight up to constant factors.\nFormal statement.\nPinsker's inequality states that, if formula_1 and formula_2 are two &lt;a href=\"probability%20distribution\"&gt;probability distribution&lt;/a&gt;s on a &lt;a href=\"measurable%20space\"&gt;measurable space&lt;/a&gt; formula_3, then\nwhere\nis the &lt;a href=\"Total%20variation%20distance%20of%20probability%20measures\"&gt;total variation distance&lt;/a&gt; (or statistical distance) between formula_1 and formula_2 and\nis the &lt;a href=\"Kullback%E2%80%93Leibler%20divergence\"&gt;Kullback\u2013Leibler divergence&lt;/a&gt; in &lt;a href=\"Nat%20%28unit%29\"&gt;nats&lt;/a&gt;. When the sample space formula_9 is a finite set, the Kullback\u2013Leibler divergence is given by\nNote that in terms of the &lt;a href=\"Total_variation%23Total_variation_of_a_measure\"&gt;total variation norm&lt;/a&gt; formula_11 of the &lt;a href=\"signed%20measure\"&gt;signed measure&lt;/a&gt; formula_12, Pinsker's inequality differs from the one given above by a factor of two:\nA proof of Pinsker's inequality uses the &lt;a href=\"partition%20inequality\"&gt;partition inequality&lt;/a&gt; for &lt;a href=\"f-divergence\"&gt;\"f\"-divergences&lt;/a&gt;.\nAlternative version.\nNote that the expression of Pinsker inequality depends on what basis of logarithm is used in the definition of KL-divergence. formula_14 is defined using formula_15 (logarithm in base formula_16), whereas formula_17 is typically defined with formula_18 (logarithm in base 2). Then,\nGiven the above comments, there is an alternative statement of Pinsker's inequality in some literature that relates &lt;a href=\"Kullback%E2%80%93Leibler%20divergence\"&gt;information divergence&lt;/a&gt; to variation distance: \ni.e.,\nin which \nis the &lt;a href=\"total%20variation\"&gt;(non-normalized) variation distance&lt;/a&gt; between two &lt;a href=\"probability%20density%20function\"&gt;probability density functions&lt;/a&gt; formula_23 and formula_24 on the same alphabet formula_25.\nThis form of Pinsker's inequality shows that \"convergence in divergence\" is stronger notion than \"convergence in variation distance\".\nHistory.\nPinsker first proved the inequality with a greater constant. The inequality in the above form was proved independently by &lt;a href=\"Solomon%20Kullback\"&gt;Kullback&lt;/a&gt;, &lt;a href=\"Imre%20Csisz%C3%A1r\"&gt;Csisz\u00e1r&lt;/a&gt;, and &lt;a href=\"Johannes%20Kemperman\"&gt;Kemperman&lt;/a&gt;.\nInverse problem.\nA precise inverse of the inequality cannot hold: for every formula_26, there are distributions formula_27 with formula_28 but formula_29. An easy example is given by the two-point space formula_30 with formula_31 and formula_32. \nHowever, an inverse inequality holds on finite spaces formula_9 with a constant depending on formula_2. More specifically, it can be shown that with the definition formula_35 we have for any measure formula_1 which is absolutely continuous to formula_2\nAs a consequence, if formula_2 has full &lt;a href=\"Support_%28measure_theory%29\"&gt;support&lt;/a&gt; (i.e. formula_40 for all formula_41), then"
}