{
    "id": "351517",
    "revid": "1068983036",
    "url": "https://en.wikipedia.org/wiki?curid=351517",
    "title": "Systolic array",
    "text": "In &lt;a href=\"parallel%20computing\"&gt;parallel&lt;/a&gt; &lt;a href=\"computer%20architectures\"&gt;computer architectures&lt;/a&gt;, a systolic array is a homogeneous &lt;a href=\"Graph%20%28discrete%20mathematics%29\"&gt;network&lt;/a&gt; of tightly coupled &lt;a href=\"data%20processing%20unit\"&gt;data processing unit&lt;/a&gt;s (DPUs) called cells or &lt;a href=\"Node%20%28computer%20science%29\"&gt;node&lt;/a&gt;s. Each node or DPU independently computes a partial result as a function of the data received from its upstream neighbours, stores the result within itself and passes it downstream. Systolic arrays were first used in &lt;a href=\"Colossus%20computer\"&gt;Colossus&lt;/a&gt;, which was an early computer used to break German &lt;a href=\"Lorenz%20cipher\"&gt;Lorenz&lt;/a&gt; ciphers during &lt;a href=\"World%20War%20II\"&gt;World War II&lt;/a&gt;. Due to the classified nature of Colossus, they were independently invented or rediscovered by &lt;a href=\"H.%20T.%20Kung\"&gt;H. T. Kung&lt;/a&gt; and &lt;a href=\"Charles%20Leiserson\"&gt;Charles Leiserson&lt;/a&gt; who described arrays for many dense linear algebra computations (matrix product, solving systems of &lt;a href=\"linear%20equation\"&gt;linear equation&lt;/a&gt;s, &lt;a href=\"LU%20decomposition\"&gt;LU decomposition&lt;/a&gt;, etc.) for banded matrices. Early applications include computing &lt;a href=\"greatest%20common%20divisor\"&gt;greatest common divisor&lt;/a&gt;s of integers and polynomials. They are sometimes classified as &lt;a href=\"Multiple%20instruction%2C%20single%20data\"&gt;multiple-instruction single-data&lt;/a&gt; (MISD) architectures under &lt;a href=\"Flynn%27s%20taxonomy\"&gt;Flynn's taxonomy&lt;/a&gt;, but this classification is questionable because a strong argument can be made to distinguish systolic arrays from any of Flynn's four categories: &lt;a href=\"Single%20instruction%2C%20single%20data\"&gt;SISD&lt;/a&gt;, &lt;a href=\"Single%20instruction%2C%20multiple%20data\"&gt;SIMD&lt;/a&gt;, &lt;a href=\"Multiple%20instruction%2C%20single%20data\"&gt;MISD&lt;/a&gt;, &lt;a href=\"Multiple%20instruction%2C%20multiple%20data\"&gt;MIMD&lt;/a&gt;, as discussed later in this article.\nThe parallel input &lt;a href=\"data\"&gt;data&lt;/a&gt; flows through a network of hard-wired &lt;a href=\"Microprocessor\"&gt;processor&lt;/a&gt; nodes, which combine, process, &lt;a href=\"merge%20algorithm\"&gt;merge&lt;/a&gt; or &lt;a href=\"sorting%20algorithm\"&gt;sort&lt;/a&gt; the input data into a derived result. Because the &lt;a href=\"wave\"&gt;wave&lt;/a&gt;-like propagation of data through a systolic array resembles the &lt;a href=\"pulse\"&gt;pulse&lt;/a&gt; of the human circulatory system, the name \"systolic\" was coined from medical terminology. The name is derived from &lt;a href=\"systole\"&gt;systole&lt;/a&gt; as an analogy to the regular pumping of blood by the heart.\nApplications.\nSystolic arrays are often hard-wired for specific operations, such as \"multiply and accumulate\", to perform massively &lt;a href=\"parallel%20computing\"&gt;parallel&lt;/a&gt; integration, &lt;a href=\"convolution\"&gt;convolution&lt;/a&gt;, &lt;a href=\"correlation\"&gt;correlation&lt;/a&gt;, &lt;a href=\"matrix%20multiplication\"&gt;matrix multiplication&lt;/a&gt; or data sorting tasks. They are also used for &lt;a href=\"dynamic%20programming\"&gt;dynamic programming&lt;/a&gt; algorithms, used in DNA and protein &lt;a href=\"sequence%20analysis\"&gt;sequence analysis&lt;/a&gt;.\nArchitecture.\nA systolic array typically consists of a large &lt;a href=\"monolithic%20system\"&gt;monolithic&lt;/a&gt; &lt;a href=\"Graph%20%28discrete%20mathematics%29\"&gt;network&lt;/a&gt; of primitive computing &lt;a href=\"Node%20%28computer%20science%29\"&gt;node&lt;/a&gt;s which can be hardwired or software configured for a specific application. The nodes are usually fixed and identical, while the interconnect is programmable. The more general wave front processors, by contrast, employ sophisticated and individually programmable nodes which may or may not be monolithic, depending on the array size and design parameters. The other distinction is that systolic arrays rely on &lt;a href=\"synchronous\"&gt;synchronous&lt;/a&gt; data transfers, while &lt;a href=\"wavefront\"&gt;wavefront&lt;/a&gt; tend to work &lt;a href=\"wikt%3Aasynchronous\"&gt;asynchronous&lt;/a&gt;ly.\nUnlike the more common &lt;a href=\"Von%20Neumann%20architecture\"&gt;Von Neumann architecture&lt;/a&gt;, where program execution follows a script of instructions stored in common memory, &lt;a href=\"address%20space\"&gt;addressed&lt;/a&gt; and sequenced under the control of the &lt;a href=\"CPU\"&gt;CPU&lt;/a&gt;'s &lt;a href=\"program%20counter\"&gt;program counter&lt;/a&gt; (PC), the individual nodes within a systolic array are triggered by the arrival of new data and always process the data in exactly the same way. The actual processing within each node may be hard wired or block &lt;a href=\"microcode\"&gt;micro code&lt;/a&gt;d, in which case the common node personality can be block programmable.\nThe systolic array paradigm with data-streams driven by data &lt;a href=\"Counter%20%28digital%29\"&gt;counter&lt;/a&gt;s, is the counterpart of the Von Neumann architecture with instruction-stream driven by a program counter. Because a systolic array usually sends and receives multiple data streams, and multiple data counters are needed to generate these data streams, it supports &lt;a href=\"data%20parallelism\"&gt;data parallelism&lt;/a&gt;.\nGoals and benefits.\nA major benefit of systolic arrays is that all operand data and partial results are stored within (passing through) the processor array. There is no need to access external buses, main memory or internal caches during each operation as is the case with Von Neumann or &lt;a href=\"Harvard%20architecture\"&gt;Harvard&lt;/a&gt; sequential machines. The sequential limits on &lt;a href=\"parallel%20computing\"&gt;parallel&lt;/a&gt; performance dictated by &lt;a href=\"Amdahl%27s%20Law\"&gt;Amdahl's Law&lt;/a&gt; also do not apply in the same way, because data dependencies are implicitly handled by the programmable &lt;a href=\"Node%20%28computer%20science%29\"&gt;node&lt;/a&gt; interconnect and there are no sequential steps in managing the highly parallel data flow.\nSystolic arrays are therefore extremely good at artificial intelligence, image processing, pattern recognition, computer vision and other tasks which animal brains do so particularly well. Wavefront processors in general can also be very good at machine learning by implementing self configuring neural nets in hardware.\nClassification controversy.\nWhile systolic arrays are officially classified as &lt;a href=\"Multiple%20instruction%2C%20single%20data\"&gt;MISD&lt;/a&gt;, their classification is somewhat problematic. Because the input is typically a vector\nof independent values, the systolic array is definitely not &lt;a href=\"Single%20instruction%2C%20single%20data\"&gt;SISD&lt;/a&gt;. Since these &lt;a href=\"input%20%28computer%20science%29\"&gt;input&lt;/a&gt; values are merged and combined into the result(s) and do not maintain their &lt;a href=\"independence\"&gt;independence&lt;/a&gt; as they would in a &lt;a href=\"Single%20instruction%2C%20multiple%20data\"&gt;SIMD&lt;/a&gt; vector processing unit, the &lt;a href=\"array%20data%20structure\"&gt;array&lt;/a&gt; cannot be classified as such. Consequently, the array cannot be classified as a &lt;a href=\"Multiple%20instruction%2C%20multiple%20data\"&gt;MIMD&lt;/a&gt; either, because MIMD can be viewed as a mere collection of smaller SISD and &lt;a href=\"Single%20instruction%2C%20multiple%20data\"&gt;SIMD&lt;/a&gt; machines.\nFinally, because the data &lt;a href=\"swarm\"&gt;swarm&lt;/a&gt; is transformed as it passes through the array from &lt;a href=\"Node%20%28computer%20science%29\"&gt;node&lt;/a&gt; to node, the multiple nodes are not operating on the same data, which makes the MISD classification a &lt;a href=\"misnomer\"&gt;misnomer&lt;/a&gt;. The other reason why a systolic array should not qualify as a MISD is the same as the one which disqualifies it from the SISD category: The input data is typically a vector not a single data value, although one could argue that any given input vector is a single item of data.\nIn spite of all of the above, systolic arrays are often offered as a classic example of MISD architecture in textbooks on &lt;a href=\"parallel%20computing\"&gt;parallel computing&lt;/a&gt; and in engineering classes. If the array is viewed from the outside as &lt;a href=\"atomic%20operation\"&gt;atomic&lt;/a&gt; it should perhaps be classified as SFMuDMeR = Single Function, Multiple Data, Merged Result(s).\nSystolic arrays use a pre-defined computational flow graph that connects their nodes. &lt;a href=\"Kahn%20process%20networks\"&gt;Kahn process networks&lt;/a&gt; use a similar flow graph, but are distinguished by the nodes working in lock-step in the systolic array: in a Kahn network, there are FIFO queues\nbetween each node.\nDetailed description.\nA systolic array is composed of matrix-like rows of &lt;a href=\"data%20processing%20unit\"&gt;data processing unit&lt;/a&gt;s called cells. Data processing units (DPUs) are similar to &lt;a href=\"central%20processing%20unit\"&gt;central processing unit&lt;/a&gt;s (CPUs), (except for the usual lack of a &lt;a href=\"program%20counter\"&gt;program counter&lt;/a&gt;, since operation is &lt;a href=\"transport%20triggered%20architecture\"&gt;transport-triggered&lt;/a&gt;, i.e., by the arrival of a data object). Each cell shares the information with its neighbors immediately after processing. The systolic array is often rectangular where data flows across the array between neighbour &lt;a href=\"data%20processing%20unit\"&gt;DPU&lt;/a&gt;s, often with different data flowing in different directions. The data streams entering and leaving the ports of the array are generated by auto-sequencing memory units, ASMs. Each ASM includes a data &lt;a href=\"Counter%20%28digital%29\"&gt;counter&lt;/a&gt;. In &lt;a href=\"embedded%20system\"&gt;embedded system&lt;/a&gt;s a data stream may also be input from and/or output to an external source.\nAn example of a systolic &lt;a href=\"algorithm\"&gt;algorithm&lt;/a&gt; might be designed for &lt;a href=\"matrix%20multiplication\"&gt;matrix multiplication&lt;/a&gt;. One &lt;a href=\"matrix%20%28math%29\"&gt;matrix&lt;/a&gt; is fed in a row at a time from the top of the array and is passed down the array, the other matrix is fed in a column at a time from the left hand side of the array and passes from left to right. Dummy values are then passed in until each processor has seen one whole row and one whole column. At this point, the result of the multiplication is stored in the array and can now be output a row or a column at a time, flowing down or across the array.\nSystolic arrays are arrays of &lt;a href=\"data%20processing%20unit\"&gt;DPU&lt;/a&gt;s which are connected to a small number of nearest neighbour DPUs in a mesh-like topology. DPUs perform a sequence of operations on data that flows between them. Because the traditional systolic array synthesis methods have been practiced by algebraic algorithms, only uniform arrays with only linear pipes can be obtained, so that the architectures are the same in all DPUs. The consequence is, that only applications with regular data dependencies can be implemented on classical systolic arrays. Like &lt;a href=\"Single%20instruction%2C%20multiple%20data\"&gt;SIMD&lt;/a&gt; machines, clocked systolic arrays compute in \"lock-step\" with each processor undertaking alternate compute | communicate phases. But systolic arrays with asynchronous handshake between DPUs are called \"wavefront arrays\".\nOne well-known systolic array is Carnegie Mellon University's &lt;a href=\"iWarp\"&gt;iWarp&lt;/a&gt; processor, which has been manufactured by Intel. An iWarp system has a linear array processor connected by data buses going in both directions.\nHistory.\nSystolic arrays (also known as \"wavefront processors\"), were first described by &lt;a href=\"H.%20T.%20Kung\"&gt;H. T. Kung&lt;/a&gt; and &lt;a href=\"Charles%20E.%20Leiserson\"&gt;Charles E. Leiserson&lt;/a&gt;, who published the first paper describing systolic arrays in 1979. However, the first machine known to have used a similar technique was the &lt;a href=\"Colossus%20computer\"&gt;Colossus Mark II&lt;/a&gt; in 1944.\nApplication example.\n&lt;a href=\"Horner%27s%20rule\"&gt;Horner's rule&lt;/a&gt; for evaluating a polynomial is:\nA linear systolic array in which the processors are arranged in pairs: one multiplies its input by formula_2 and passes the result to the right, the next adds formula_3 and passes the result to the right.\nAdvantages and disadvantages.\nPros\nCons"
}