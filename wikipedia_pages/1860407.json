{
    "id": "1860407",
    "revid": "7903804",
    "url": "https://en.wikipedia.org/wiki?curid=1860407",
    "title": "K-means clustering",
    "text": "\"k\"-means clustering is a method of &lt;a href=\"vector%20quantization\"&gt;vector quantization&lt;/a&gt;, originally from &lt;a href=\"signal%20processing\"&gt;signal processing&lt;/a&gt;, that aims to &lt;a href=\"Partition%20of%20a%20set\"&gt;partition&lt;/a&gt; \"n\" observations into \"k\" clusters in which each observation belongs to the &lt;a href=\"Cluster%20%28statistics%29\"&gt;cluster&lt;/a&gt; with the nearest &lt;a href=\"mean\"&gt;mean&lt;/a&gt; (cluster centers or cluster &lt;a href=\"centroid\"&gt;centroid&lt;/a&gt;), serving as a prototype of the cluster. This results in a partitioning of the data space into &lt;a href=\"Voronoi%20cell\"&gt;Voronoi cell&lt;/a&gt;s. \"k\"-means clustering minimizes within-cluster variances (&lt;a href=\"squared%20Euclidean%20distance\"&gt;squared Euclidean distance&lt;/a&gt;s), but not regular Euclidean distances, which would be the more difficult &lt;a href=\"Weber%20problem\"&gt;Weber problem&lt;/a&gt;: the mean optimizes squared errors, whereas only the &lt;a href=\"geometric%20median\"&gt;geometric median&lt;/a&gt; minimizes Euclidean distances. For instance, better Euclidean solutions can be found using &lt;a href=\"K-medians%20clustering\"&gt;k-medians&lt;/a&gt; and &lt;a href=\"k-medoids\"&gt;k-medoids&lt;/a&gt;.\nThe problem is computationally difficult (&lt;a href=\"NP-hardness\"&gt;NP-hard&lt;/a&gt;); however, efficient &lt;a href=\"heuristic%20algorithm\"&gt;heuristic algorithm&lt;/a&gt;s converge quickly to a &lt;a href=\"local%20optimum\"&gt;local optimum&lt;/a&gt;. These are usually similar to the &lt;a href=\"expectation-maximization%20algorithm\"&gt;expectation-maximization algorithm&lt;/a&gt; for &lt;a href=\"Mixture%20model\"&gt;mixtures&lt;/a&gt; of &lt;a href=\"Gaussian%20distribution\"&gt;Gaussian distribution&lt;/a&gt;s via an iterative refinement approach employed by both \"k-means\" and \"Gaussian mixture modeling\". They both use cluster centers to model the data; however, \"k\"-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.\nThe unsupervised k-means algorithm has a loose relationship to the &lt;a href=\"K-nearest%20neighbor\"&gt;\"k\"-nearest neighbor classifier&lt;/a&gt;, a popular supervised &lt;a href=\"machine%20learning\"&gt;machine learning&lt;/a&gt; technique for classification that is often confused with \"k\"-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by \"k\"-means classifies new data into the existing clusters. This is known as &lt;a href=\"nearest%20centroid%20classifier\"&gt;nearest centroid classifier&lt;/a&gt; or &lt;a href=\"Rocchio%20algorithm\"&gt;Rocchio algorithm&lt;/a&gt;.\nDescription.\nGiven a set of observations (x1, x2, ..., x\"n\"), where each observation is a \"d\"-dimensional real vector, \"k\"-means clustering aims to partition the \"n\" observations into \"k\" (\u2264\u00a0\"n\") sets S\u00a0=\u00a0{\"S\"1,\u00a0\"S\"2,\u00a0...,\u00a0\"Sk\"} so as to minimize the within-cluster sum of squares (WCSS) (i.e. &lt;a href=\"variance\"&gt;variance&lt;/a&gt;). Formally, the objective is to find:formula_1where \"\u03bci\" is the mean of points in \"Si\". This is equivalent to minimizing the pairwise squared deviations of points in the same cluster:formula_2The equivalence can be deduced from identity formula_3. Because the total variance is constant, this is equivalent to maximizing the sum of squared deviations between points in \"different\" clusters (between-cluster sum of squares, BCSS), which follows from the &lt;a href=\"law%20of%20total%20variance\"&gt;law of total variance&lt;/a&gt;.\nHistory.\nThe term \"\"k\"-means\" was first used by James MacQueen in 1967, though the idea goes back to &lt;a href=\"Hugo%20Steinhaus\"&gt;Hugo Steinhaus&lt;/a&gt; in 1956. The standard algorithm was first proposed by Stuart Lloyd of &lt;a href=\"Bell%20Labs\"&gt;Bell Labs&lt;/a&gt; in 1957 as a technique for &lt;a href=\"pulse-code%20modulation\"&gt;pulse-code modulation&lt;/a&gt;, although it was not published as a journal article until 1982. In 1965, Edward W. Forgy published essentially the same method, which is why it is sometimes referred to as the Lloyd\u2013Forgy algorithm.\nAlgorithms.\nStandard algorithm (naive k-means).\nThe most common algorithm uses an iterative refinement technique. Due to its ubiquity, it is often called \"the \"k\"-means algorithm\"; it is also referred to as &lt;a href=\"Lloyd%27s%20algorithm\"&gt;Lloyd's algorithm&lt;/a&gt;, particularly in the computer science community. It is sometimes also referred to as \"na\u00efve \"k\"-means\", because there exist much faster alternatives.\nGiven an initial set of \"k\" means \"m\"1(1)...,\"mk\"(1) (see below), the algorithm proceeds by alternating between two steps:\nThe algorithm has converged when the assignments no longer change. The algorithm is not guaranteed to find the optimum.\nThe algorithm is often presented as assigning objects to the nearest cluster by distance. Using a different distance function other than (squared) Euclidean distance may prevent the algorithm from converging. Various modifications of \"k\"-means such as spherical \"k\"-means and &lt;a href=\"K-medoids\"&gt;\"k\"-medoids&lt;/a&gt; have been proposed to allow using other distance measures.\nInitialization methods.\nCommonly used initialization methods are Forgy and Random Partition. The Forgy method randomly chooses \"k\" observations from the dataset and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the cluster's randomly assigned points. The Forgy method tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. According to Hamerly et al., the Random Partition method is generally preferable for algorithms such as the \"k\"-harmonic means and fuzzy \"k\"-means. For expectation maximization and standard \"k\"-means algorithms, the Forgy method of initialization is preferable. A comprehensive study by Celebi et al., however, found that popular initialization methods such as Forgy, Random Partition, and Maximin often perform poorly, whereas Bradley and Fayyad's approach performs \"consistently\" in \"the best group\" and &lt;a href=\"K-means%2B%2B\"&gt;\"k\"-means++&lt;/a&gt; performs \"generally well\".\nThe algorithm does not guarantee convergence to the global optimum. The result may depend on the initial clusters. As the algorithm is usually fast, it is common to run it multiple times with different starting conditions. However, worst-case performance can be slow: in particular certain point sets, even in two dimensions, converge in exponential time, that is . These point sets do not seem to arise in practice: this is corroborated by the fact that the &lt;a href=\"Smoothed%20analysis\"&gt;smoothed&lt;/a&gt; running time of \"k\"-means is polynomial.\nThe \"assignment\" step is referred to as the \"expectation step\", while the \"update step\" is a maximization step, making this algorithm a variant of the \"generalized\" &lt;a href=\"expectation-maximization%20algorithm\"&gt;expectation-maximization algorithm&lt;/a&gt;.\nComplexity.\nFinding the optimal solution to the \"k\"-means clustering problem for observations in \"d\" dimensions is:\nThus, a variety of &lt;a href=\"heuristic%20algorithm\"&gt;heuristic algorithm&lt;/a&gt;s such as Lloyd's algorithm given above are generally used.\nThe running time of Lloyd's algorithm (and most variants) is formula_9, where:\nOn data that does have a clustering structure, the number of iterations until convergence is often small, and results only improve slightly after the first dozen iterations. Lloyd's algorithm is therefore often considered to be of \"linear\" complexity in practice, although it is in the &lt;a href=\"Worst-case%20complexity\"&gt;worst case&lt;/a&gt; superpolynomial when performed until convergence.\nLloyd's algorithm is the standard approach for this problem. However, it spends a lot of processing time computing the distances between each of the k cluster centers and the n data points. Since points usually stay in the same clusters after a few iterations, much of this work is unnecessary, making the na\u00efve implementation very inefficient. Some implementations use caching and the triangle inequality in order to create bounds and accelerate Lloyd's algorithm.\nHartigan\u2013Wong method.\nHartigan and Wong's method provides a variation of \"k\"-means algorithm which progresses towards a local minimum of the minimum sum-of-squares problem with different solution updates. The method is a &lt;a href=\"Local%20search%20%28optimization%29\"&gt;local search&lt;/a&gt; that iteratively attempts to relocate a sample into a different cluster as long as this process improves the objective function. When no sample can be relocated into a different cluster with an improvement of the objective, the method stops (in a local minimum). In a similar way as the classical \"k\"-means, the approach remains a heuristic since it does not necessarily guarantee that the final solution is globally optimum.\nLet formula_18 be the individual cost of formula_19 defined by formula_20, with formula_21 the center of the cluster.\nAssignment step: Hartigan and Wong's method starts by partitioning the points into random clusters formula_22.\nUpdate step: Next it determines the formula_23 and formula_24 for which the following function reaches a maximum\nFor the formula_26 that reach this maximum, formula_27 moves from the cluster formula_28 to the cluster formula_29.\nTermination: The algorithm terminates once formula_30 is less than zero for all formula_26.\nDifferent move acceptance strategies can be used. In a \"first-improvement\" strategy, any improving relocation can be applied, whereas in a \"best-improvement\" strategy, all possible relocations are iteratively tested and only the best is applied at each iteration. The former approach favors speed, whether the latter approach generally favors solution quality at the expense of additional computational time. The function formula_32 used to calculate the result of a relocation can also be efficiently evaluated by using equality\nGlobal optimization and metaheuristics.\nThe classical k-means algorithm and its variations are known to only converge to local minima of the minimum-sum-of-squares clustering problem defined as formula_34Many studies have attempted to improve the convergence behavior of the algorithm and maximize the chances of attaining the global optimum (or at least, local minima of better quality). Initialization and restart techniques discussed in the previous sections are one alternative to find better solutions. More recently, mathematical programming algorithms based on &lt;a href=\"branch-and-bound\"&gt;branch-and-bound&lt;/a&gt; and &lt;a href=\"column%20generation\"&gt;column generation&lt;/a&gt; have produced \u2018\u2019provenly optimal\u2019\u2019 solutions for datasets with up to 2,300 entities. As expected, due to the &lt;a href=\"NP-hardness\"&gt;NP-hardness&lt;/a&gt; of the subjacent optimization problem, the computational time of optimal algorithms for K-means quickly increases beyond this size. Optimal solutions for small- and medium-scale still remain valuable as a benchmark tool, to evaluate the quality of other heuristics. To find high-quality local minima within a controlled computational time but without optimality guarantees, other works have explored &lt;a href=\"metaheuristics\"&gt;metaheuristics&lt;/a&gt; and other &lt;a href=\"global%20optimization\"&gt;global optimization&lt;/a&gt; techniques, e.g., based on incremental approaches and convex optimization, random swaps (i.e., &lt;a href=\"iterated%20local%20search\"&gt;iterated local search&lt;/a&gt;), &lt;a href=\"variable%20neighborhood%20search\"&gt;variable neighborhood search&lt;/a&gt;and &lt;a href=\"genetic%20algorithms\"&gt;genetic algorithms&lt;/a&gt;. It is indeed known that finding better local minima of the minimum sum-of-squares clustering problem can make the difference between failure and success to recover cluster structures in feature spaces of high dimension.\nDiscussion.\nThree key features of \"k\"-means that make it efficient are often regarded as its biggest drawbacks:\nA key limitation of \"k\"-means is its cluster model. The concept is based on spherical clusters that are separable so that the mean converges towards the cluster center. The clusters are expected to be of similar size, so that the assignment to the nearest cluster center is the correct assignment. When for example applying \"k\"-means with a value of formula_35 onto the well-known &lt;a href=\"Iris%20flower%20data%20set\"&gt;Iris flower data set&lt;/a&gt;, the result often fails to separate the three &lt;a href=\"Iris%20%28plant%29\"&gt;Iris&lt;/a&gt; species contained in the data set. With formula_36, the two visible clusters (one containing two species) will be discovered, whereas with formula_35 one of the two clusters will be split into two even parts. In fact, formula_36 is more appropriate for this data set, despite the data set's containing 3 \"classes\". As with any other clustering algorithm, the \"k\"-means result makes assumptions that the data satisfy certain criteria. It works well on some data sets, and fails on others.\nThe result of \"k\"-means can be seen as the &lt;a href=\"Voronoi%20diagram\"&gt;Voronoi cells&lt;/a&gt; of the cluster means. Since data is split halfway between cluster means, this can lead to suboptimal splits as can be seen in the \"mouse\" example. The Gaussian models used by the &lt;a href=\"expectation-maximization%20algorithm\"&gt;expectation-maximization algorithm&lt;/a&gt; (arguably a generalization of \"k\"-means) are more flexible by having both variances and covariances. The EM result is thus able to accommodate clusters of variable size much better than \"k\"-means as well as correlated clusters (not in this example). In counterpart, EM requires the optimization of a larger number of free parameters and poses some methodological issues due to vanishing clusters or badly-conditioned covariance matrices. \"K\"-means is closely related to nonparametric &lt;a href=\"Bayesian%20inference\"&gt;Bayesian modeling&lt;/a&gt;.\nApplications.\n\"k\"-means clustering is rather easy to apply to even large data sets, particularly when using heuristics such as &lt;a href=\"Lloyd%27s%20algorithm\"&gt;Lloyd's algorithm&lt;/a&gt;. It has been successfully used in &lt;a href=\"market%20segmentation\"&gt;market segmentation&lt;/a&gt;, &lt;a href=\"computer%20vision\"&gt;computer vision&lt;/a&gt;, and &lt;a href=\"astronomy\"&gt;astronomy&lt;/a&gt; among many other domains. It often is used as a preprocessing step for other algorithms, for example to find a starting configuration.\nVector quantization.\n\"k\"-means originates from signal processing, and still finds use in this domain. For example, in &lt;a href=\"computer%20graphics\"&gt;computer graphics&lt;/a&gt;, &lt;a href=\"color%20quantization\"&gt;color quantization&lt;/a&gt; is the task of reducing the &lt;a href=\"Palette%20%28computing%29\"&gt;color palette&lt;/a&gt; of an image to a fixed number of colors \"k\". The \"k\"-means algorithm can easily be used for this task and produces competitive results. A use case for this approach is &lt;a href=\"image%20segmentation\"&gt;image segmentation&lt;/a&gt;. Other uses of vector quantization include &lt;a href=\"Sampling%20%28statistics%29\"&gt;non-random sampling&lt;/a&gt;, as \"k\"-means can easily be used to choose \"k\" different but prototypical objects from a large data set for further analysis.\nCluster analysis.\nIn cluster analysis, the \"k\"-means algorithm can be used to partition the input data set into \"k\" partitions (clusters).\nHowever, the pure \"k\"-means algorithm is not very flexible, and as such is of limited use (except for when vector quantization as above is actually the desired use case). In particular, the parameter \"k\" is known to be hard to choose (as discussed above) when not given by external constraints. Another limitation is that it cannot be used with arbitrary distance functions or on non-numerical data. For these use cases, many other algorithms are superior.\nFeature learning.\n\"k\"-means clustering has been used as a &lt;a href=\"feature%20learning\"&gt;feature learning&lt;/a&gt; (or &lt;a href=\"dictionary%20learning\"&gt;dictionary learning&lt;/a&gt;) step, in either (&lt;a href=\"Semi-supervised%20learning\"&gt;semi-&lt;/a&gt;)&lt;a href=\"supervised%20learning\"&gt;supervised learning&lt;/a&gt; or &lt;a href=\"unsupervised%20learning\"&gt;unsupervised learning&lt;/a&gt;. The basic approach is first to train a \"k\"-means clustering representation, using the input training data (which need not be labelled). Then, to project any input datum into the new feature space, an \"encoding\" function, such as the thresholded matrix-product of the datum with the centroid locations, computes the distance from the datum to each centroid, or simply an indicator function for the nearest centroid, or some smooth transformation of the distance. Alternatively, transforming the sample-cluster distance through a &lt;a href=\"Radial%20basis%20function\"&gt;Gaussian RBF&lt;/a&gt;, obtains the hidden layer of a &lt;a href=\"radial%20basis%20function%20network\"&gt;radial basis function network&lt;/a&gt;.\nThis use of \"k\"-means has been successfully combined with simple, &lt;a href=\"linear%20classifier\"&gt;linear classifier&lt;/a&gt;s for semi-supervised learning in &lt;a href=\"Natural%20language%20processing\"&gt;NLP&lt;/a&gt; (specifically for &lt;a href=\"named%20entity%20recognition\"&gt;named entity recognition&lt;/a&gt;) and in &lt;a href=\"computer%20vision\"&gt;computer vision&lt;/a&gt;. On an object recognition task, it was found to exhibit comparable performance with more sophisticated feature learning approaches such as &lt;a href=\"autoencoder\"&gt;autoencoder&lt;/a&gt;s and &lt;a href=\"restricted%20Boltzmann%20machine\"&gt;restricted Boltzmann machine&lt;/a&gt;s. However, it generally requires more data, for equivalent performance, because each data point only contributes to one \"feature\".\nRelation to other algorithms.\nGaussian mixture model.\nThe slow \"standard algorithm\" for \"k\"-means clustering, and its associated &lt;a href=\"Expectation%E2%80%93maximization%20algorithm\"&gt;expectation-maximization algorithm&lt;/a&gt;, is a special case of a Gaussian mixture model, specifically, the limiting case when fixing all covariances to be diagonal, equal and have infinitesimal small variance. Instead of small variances, a hard cluster assignment can also be used to show another equivalence of \"k\"-means clustering to a special case of \"hard\" Gaussian mixture modelling. This does not mean that it is efficient to use Gaussian mixture modelling to compute \"k\"-means, but just that there is a theoretical relationship, and that Gaussian mixture modelling can be interpreted as a generalization of \"k\"-means; on the contrary, it has been suggested to use k-means clustering to find starting points for Gaussian mixture modelling on difficult data.\nK-SVD.\nAnother generalization of the \"k\"-means algorithm is the K-SVD algorithm, which estimates data points as a sparse linear combination of \"codebook vectors\". \"k\"-means corresponds to the special case of using a single codebook vector, with a weight of 1.\nPrincipal component analysis.\nThe relaxed solution of -means clustering, specified by the cluster indicators, is given by principal component analysis (PCA). The intuition is that \"k\"-means describe spherically shaped (ball-like) clusters. If the data has 2 clusters, the line connecting the two centroids is the best 1-dimensional projection direction, which is also the first PCA direction. Cutting the line at the center of mass separates the clusters (this is the continuous relaxation of the discrete cluster indicator). If the data have three clusters, the 2-dimensional plane spanned by three cluster centroids is the best 2-D projection. This plane is also defined by the first two PCA dimensions. Well-separated clusters are effectively modelled by ball-shaped clusters and thus discovered by \"k\"-means. Non-ball-shaped clusters are hard to separate when they are close. For example, two half-moon shaped clusters intertwined in space do not separate well when projected onto PCA subspace. \"k\"-means should not be expected to do well on this data. It is straightforward to produce counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.\nMean shift clustering.\nBasic mean shift clustering algorithms maintain a set of data points the same size as the input data set. Initially, this set is copied from the input set. Then this set is iteratively replaced by the mean of those points in the set that are within a given distance of that point. By contrast, \"k\"-means restricts this updated set to \"k\" points usually much less than the number of points in the input data set, and replaces each point in this set by the mean of all points in the \"input set\" that are closer to that point than any other (e.g. within the Voronoi partition of each updating point). A mean shift algorithm that is similar then to \"k\"-means, called \"likelihood mean shift\", replaces the set of points undergoing replacement by the mean of all points in the input set that are within a given distance of the changing set. One of the advantages of mean shift over \"k\"-means is that the number of clusters is not pre-specified, because mean shift is likely to find only a few clusters if only a small number exist. However, mean shift can be much slower than \"k\"-means, and still requires selection of a bandwidth parameter. Mean shift has soft variants.\nIndependent component analysis.\nUnder sparsity assumptions and when input data is pre-processed with the &lt;a href=\"whitening%20transformation\"&gt;whitening transformation&lt;/a&gt;, \"k\"-means produces the solution to the linear independent component analysis (ICA) task. This aids in explaining the successful application of \"k\"-means to &lt;a href=\"%23Feature%20learning\"&gt;feature learning&lt;/a&gt;.\nBilateral filtering.\n\"k\"-means implicitly assumes that the ordering of the input data set does not matter. The bilateral filter is similar to \"k\"-means and &lt;a href=\"mean%20shift\"&gt;mean shift&lt;/a&gt; in that it maintains a set of data points that are iteratively replaced by means. However, the bilateral filter restricts the calculation of the (kernel weighted) mean to include only points that are close in the ordering of the input data. This makes it applicable to problems such as image denoising, where the spatial arrangement of pixels in an image is of critical importance.\nSimilar problems.\nThe set of squared error minimizing cluster functions also includes the &lt;a href=\"K-medoids\"&gt;\"k\"-medoids&lt;/a&gt; algorithm, an approach which forces the center point of each cluster to be one of the actual points, i.e., it uses &lt;a href=\"medoids\"&gt;medoids&lt;/a&gt; in place of &lt;a href=\"centroids\"&gt;centroids&lt;/a&gt;.\nSoftware implementations.\nDifferent implementations of the algorithm exhibit performance differences, with the fastest on a test data set finishing in 10 seconds, the slowest taking 25,988 seconds (~7 hours). The differences can be attributed to implementation quality, language and compiler differences, different termination criteria and precision levels, and the use of indexes for acceleration.\nFree Software/Open Source.\nThe following implementations are available under &lt;a href=\"Free%20and%20open-source%20software\"&gt;Free/Open Source Software&lt;/a&gt; licenses, with publicly available source code.\nProprietary.\nThe following implementations are available under &lt;a href=\"Proprietary%20software\"&gt;proprietary&lt;/a&gt; license terms, and may not have publicly available source code."
}