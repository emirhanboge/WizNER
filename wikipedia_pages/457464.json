{
    "id": "457464",
    "revid": "1056614655",
    "url": "https://en.wikipedia.org/wiki?curid=457464",
    "title": "Block matrix",
    "text": "In &lt;a href=\"mathematics\"&gt;mathematics&lt;/a&gt;, a block matrix or a partitioned matrix is a &lt;a href=\"matrix%20%28mathematics%29\"&gt;matrix&lt;/a&gt; that is \"&lt;a href=\"Interpretation%20%28logic%29\"&gt;interpreted&lt;/a&gt;\" as having been broken into sections called blocks or submatrices. Intuitively, a matrix interpreted as a block matrix can be visualized as the original matrix with a collection of horizontal and vertical lines, which break it up, or &lt;a href=\"Partition%20of%20a%20set\"&gt;partition&lt;/a&gt; it, into a collection of smaller matrices. Any matrix may be interpreted as a block matrix in one or more ways, with each interpretation defined by how its rows and columns are partitioned.\nThis notion can be made more precise for an formula_1 by formula_2 matrix formula_3 by partitioning formula_1 into a collection formula_5, and then partitioning formula_2 into a collection formula_7. The original matrix is then considered as the \"total\" of these groups, in the sense that the formula_8 entry of the original matrix corresponds in a &lt;a href=\"Bijection\"&gt;1-to-1&lt;/a&gt; way with some formula_9 &lt;a href=\"offset%20%28computer%20science%29\"&gt;offset&lt;/a&gt; entry of some formula_10, where formula_11 and formula_12.\nBlock matrix algebra arises in general from &lt;a href=\"biproduct\"&gt;biproduct&lt;/a&gt;s in &lt;a href=\"Category%20%28mathematics%29\"&gt;categories&lt;/a&gt; of matrices.\nExample.\nThe matrix\ncan be partitioned into four 2\u00d72 blocks\nThe partitioned matrix can then be written as\nBlock matrix multiplication.\nIt is possible to use a block partitioned matrix product that involves only algebra on submatrices of the factors. The partitioning of the factors is not arbitrary, however, and requires \"conformable partitions\" between two matrices formula_16 and formula_17 such that all submatrix products that will be used are defined. Given an formula_18 matrix formula_19 with formula_20 row partitions and formula_21 column partitions\nand a formula_23 matrix formula_24 with formula_21 row partitions and formula_26 column partitions\nthat are compatible with the partitions of formula_16, the matrix product\ncan be formed blockwise, yielding formula_30 as an formula_31 matrix with formula_20 row partitions and formula_26 column partitions. The matrices in the resulting matrix formula_30 are calculated by multiplying:\nOr, using the &lt;a href=\"Einstein%20notation\"&gt;Einstein notation&lt;/a&gt; that implicitly sums over repeated indices:\nBlock matrix inversion.\nIf a matrix is partitioned into four blocks, it can be &lt;a href=\"invertible%20matrix%23Blockwise%20inversion\"&gt;inverted blockwise&lt;/a&gt; as follows:\nwhere A and D are square of arbitrary size, and B and C are &lt;a href=\"conformable%20matrix\"&gt;conformable&lt;/a&gt; for partitioning. Furthermore, A and the Schur complement of A in P: must be invertible.\nEquivalently, by permuting the blocks:\nHere, D and the Schur complement of D in P: must be invertible. \nIf A and D are both invertible, then:\nBy the &lt;a href=\"Weinstein%E2%80%93Aronszajn%20identity\"&gt;Weinstein\u2013Aronszajn identity&lt;/a&gt;, one of the two matrices in the block-diagonal matrix is invertible exactly when the other is.\nBlock matrix determinant.\nThe formula for the determinant of a formula_40-matrix above continues to hold, under appropriate further assumptions, for a matrix composed of four submatrices formula_41. The easiest such formula, which can be proven using either the Leibniz formula or a factorization involving the &lt;a href=\"Schur%20complement\"&gt;Schur complement&lt;/a&gt;, is\nIf formula_16 is &lt;a href=\"Invertible%20matrix\"&gt;invertible&lt;/a&gt; (and similarly if formula_44 is invertible), one has\nIf formula_44 is a formula_47-matrix, this simplifies to formula_48.\nIf the blocks are square matrices of the \"same\" size further formulas hold. For example, if formula_49 and formula_44 &lt;a href=\"commutativity\"&gt;commute&lt;/a&gt; (i.e., formula_51), then there holds \nThis formula has been generalized to matrices composed of more than formula_40 blocks, again under appropriate commutativity conditions among the individual blocks. \nFor formula_54 and formula_55, the following formula holds (even if formula_16 and formula_17 do not commute)\nBlock diagonal matrices.\nA block diagonal matrix is a block matrix that is a &lt;a href=\"square%20matrix\"&gt;square matrix&lt;/a&gt; such that the main-diagonal blocks are square matrices and all off-diagonal blocks are zero matrices. That is, a block diagonal matrix A has the form\nwhere A\"k\" is a square matrix for all \"k\" = 1, ..., \"n\". In other words, matrix A is the &lt;a href=\"direct%20sum%20of%20matrices\"&gt;direct sum&lt;/a&gt; of A1, ..., A\"n\". It can also be indicated as A1\u00a0\u2295\u00a0A2\u00a0\u2295\u00a0...\u00a0\u2295\u00a0An \u00a0or\u00a0 diag(A1, A2, ..., An) \u00a0(the latter being the same formalism used for a &lt;a href=\"diagonal%20matrix\"&gt;diagonal matrix&lt;/a&gt;). Any square matrix can trivially be considered a block diagonal matrix with only one block.\nFor the &lt;a href=\"determinant\"&gt;determinant&lt;/a&gt; and &lt;a href=\"trace%20%28linear%20algebra%29\"&gt;trace&lt;/a&gt;, the following properties hold\nA block diagonal matrix is invertible if and only if each of its main-diagonal blocks are invertible, and in this case its inverse is another block diagonal matrix given by\nThe eigenvalues and eigenvectors of formula_16 are simply those of formula_63 and formula_64 and ... and formula_65 combined.\nBlock tridiagonal matrices.\nA block tridiagonal matrix is another special block matrix, which is just like the block diagonal matrix a &lt;a href=\"square%20matrix\"&gt;square matrix&lt;/a&gt;, having square matrices (blocks) in the lower diagonal, &lt;a href=\"main%20diagonal\"&gt;main diagonal&lt;/a&gt; and upper diagonal, with all other blocks being zero matrices. It is essentially a &lt;a href=\"tridiagonal%20matrix\"&gt;tridiagonal matrix&lt;/a&gt; but has submatrices in places of scalars. A block tridiagonal matrix A has the form\nwhere A\"k\", B\"k\" and C\"k\" are square sub-matrices of the lower, main and upper diagonal respectively.\nBlock tridiagonal matrices are often encountered in numerical solutions of engineering problems (e.g., &lt;a href=\"computational%20fluid%20dynamics\"&gt;computational fluid dynamics&lt;/a&gt;). Optimized numerical methods for &lt;a href=\"LU%20factorization\"&gt;LU factorization&lt;/a&gt; are available and hence efficient solution algorithms for equation systems with a block tridiagonal matrix as coefficient matrix. The &lt;a href=\"Thomas%20algorithm\"&gt;Thomas algorithm&lt;/a&gt;, used for efficient solution of equation systems involving a &lt;a href=\"tridiagonal%20matrix\"&gt;tridiagonal matrix&lt;/a&gt; can also be applied using matrix operations to block tridiagonal matrices (see also &lt;a href=\"Block%20LU%20decomposition\"&gt;Block LU decomposition&lt;/a&gt;).\nBlock Toeplitz matrices.\nA block Toeplitz matrix is another special block matrix, which contains blocks that are repeated down the diagonals of the matrix, as a &lt;a href=\"Toeplitz%20matrix\"&gt;Toeplitz matrix&lt;/a&gt; has elements repeated down the diagonal. \nA block Toeplitz matrix A has the form\nBlock transpose.\nA special form of matrix &lt;a href=\"transpose\"&gt;transpose&lt;/a&gt; can also be defined for block matrices, where individual blocks are reordered but not transposed. Let formula_68 be a formula_69 block matrix with formula_70 blocks formula_71, the block transpose of formula_16 is the formula_73 block matrix formula_74 with formula_70 blocks formula_76.\nAs with the conventional trace operator, the block transpose is a &lt;a href=\"linear%20mapping\"&gt;linear mapping&lt;/a&gt; such that formula_77. However, in general the property formula_78 does not hold unless the blocks of formula_16 and formula_49 commute.\nDirect sum.\nFor any arbitrary matrices A (of size \"m\"\u00a0\u00d7\u00a0\"n\") and B (of size \"p\"\u00a0\u00d7\u00a0\"q\"), we have the direct sum of A and B, denoted by A\u00a0formula_81\u00a0B and defined as \nFor instance,\nThis operation generalizes naturally to arbitrary dimensioned arrays (provided that A and B have the same number of dimensions).\nNote that any element in the &lt;a href=\"direct%20sum%20of%20vector%20spaces\"&gt;direct sum&lt;/a&gt; of two &lt;a href=\"vector%20space\"&gt;vector space&lt;/a&gt;s of matrices could be represented as a direct sum of two matrices.\nApplication.\nIn &lt;a href=\"linear%20algebra\"&gt;linear algebra&lt;/a&gt; terms, the use of a block matrix corresponds to having a &lt;a href=\"linear%20mapping\"&gt;linear mapping&lt;/a&gt; thought of in terms of corresponding 'bunches' of &lt;a href=\"basis%20vector\"&gt;basis vector&lt;/a&gt;s. That again matches the idea of having distinguished direct sum decompositions of the &lt;a href=\"domain%20of%20a%20function\"&gt;domain&lt;/a&gt; and &lt;a href=\"range%20of%20a%20function\"&gt;range&lt;/a&gt;. It is always particularly significant if a block is the &lt;a href=\"zero%20matrix\"&gt;zero matrix&lt;/a&gt;; that carries the information that a summand maps into a sub-sum.\nGiven the interpretation \"via\" linear mappings and direct sums, there is a special type of block matrix that occurs for square matrices (the case \"m\" = \"n\"). For those we can assume an interpretation as an &lt;a href=\"endomorphism\"&gt;endomorphism&lt;/a&gt; of an \"n\"-dimensional space \"V\"; the block structure in which the bunching of rows and columns is the same is of importance because it corresponds to having a single direct sum decomposition on \"V\" (rather than two). In that case, for example, the &lt;a href=\"diagonal\"&gt;diagonal&lt;/a&gt; blocks in the obvious sense are all square. This type of structure is required to describe the &lt;a href=\"Jordan%20normal%20form\"&gt;Jordan normal form&lt;/a&gt;.\nThis technique is used to cut down calculations of matrices, column-row expansions, and many &lt;a href=\"computer%20science\"&gt;computer science&lt;/a&gt; applications, including &lt;a href=\"VLSI\"&gt;VLSI&lt;/a&gt; chip design. An example is the &lt;a href=\"Strassen%20algorithm\"&gt;Strassen algorithm&lt;/a&gt; for fast &lt;a href=\"matrix%20multiplication\"&gt;matrix multiplication&lt;/a&gt;, as well as the &lt;a href=\"Hamming%287%2C4%29\"&gt;Hamming(7,4)&lt;/a&gt; encoding for error detection and recovery in data transmissions."
}