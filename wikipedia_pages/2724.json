{
    "id": "2724",
    "revid": "869314",
    "url": "https://en.wikipedia.org/wiki?curid=2724",
    "title": "Autocorrelation",
    "text": "Autocorrelation, sometimes known as serial correlation in the &lt;a href=\"discrete%20time\"&gt;discrete time&lt;/a&gt; case, is the &lt;a href=\"correlation\"&gt;correlation&lt;/a&gt; of a &lt;a href=\"Signal%20%28information%20theory%29\"&gt;signal&lt;/a&gt; with a delayed copy of itself as a function of delay. Informally, it is the similarity between observations as a function of the time lag between them. The analysis of autocorrelation is a mathematical tool for finding repeating patterns, such as the presence of a &lt;a href=\"periodic%20signal\"&gt;periodic signal&lt;/a&gt; obscured by &lt;a href=\"noise%20%28signal%20processing%29\"&gt;noise&lt;/a&gt;, or identifying the &lt;a href=\"missing%20fundamental%20frequency\"&gt;missing fundamental frequency&lt;/a&gt; in a signal implied by its &lt;a href=\"harmonic\"&gt;harmonic&lt;/a&gt; frequencies. It is often used in &lt;a href=\"signal%20processing\"&gt;signal processing&lt;/a&gt; for analyzing functions or series of values, such as &lt;a href=\"time%20domain\"&gt;time domain&lt;/a&gt; signals.\nDifferent fields of study define autocorrelation differently, and not all of these definitions are equivalent. In some fields, the term is used interchangeably with &lt;a href=\"autocovariance\"&gt;autocovariance&lt;/a&gt;.\n&lt;a href=\"Unit%20root\"&gt;Unit root&lt;/a&gt; processes, &lt;a href=\"trend-stationary%20process\"&gt;trend-stationary process&lt;/a&gt;es, &lt;a href=\"autoregressive%20process\"&gt;autoregressive process&lt;/a&gt;es, and &lt;a href=\"moving%20average%20process\"&gt;moving average process&lt;/a&gt;es are specific forms of processes with autocorrelation.\nAuto-correlation of stochastic processes.\nIn &lt;a href=\"statistics\"&gt;statistics&lt;/a&gt;, the autocorrelation of a real or complex &lt;a href=\"random%20process\"&gt;random process&lt;/a&gt; is the &lt;a href=\"Pearson%20correlation%20coefficient\"&gt;Pearson correlation&lt;/a&gt; between values of the process at different times, as a function of the two times or of the time lag. Let formula_1 be a random process, and formula_2 be any point in time (formula_2 may be an &lt;a href=\"integer\"&gt;integer&lt;/a&gt; for a &lt;a href=\"discrete-time\"&gt;discrete-time&lt;/a&gt; process or a &lt;a href=\"real%20number\"&gt;real number&lt;/a&gt; for a &lt;a href=\"continuous-time\"&gt;continuous-time&lt;/a&gt; process). Then formula_4 is the value (or &lt;a href=\"Realization%20%28probability%29\"&gt;realization&lt;/a&gt;) produced by a given &lt;a href=\"Execution%20%28computing%29\"&gt;run&lt;/a&gt; of the process at time formula_2. Suppose that the process has &lt;a href=\"mean\"&gt;mean&lt;/a&gt; formula_6 and &lt;a href=\"variance\"&gt;variance&lt;/a&gt; formula_7 at time formula_2, for each formula_2. Then the definition of the auto-correlation function between times formula_10 and formula_11 is\nwhere formula_12 is the &lt;a href=\"expected%20value\"&gt;expected value&lt;/a&gt; operator and the bar represents &lt;a href=\"complex%20conjugation\"&gt;complex conjugation&lt;/a&gt;. Note that the expectation may not be &lt;a href=\"well%20defined\"&gt;well defined&lt;/a&gt;.\nSubtracting the mean before multiplication yields the auto-covariance function between times formula_10 and formula_11:\nNote that this expression is not well defined for all time series or processes, because the mean may not exist, or the variance may be zero (for a constant process) or infinite (for processes with distribution lacking well-behaved moments, such as certain types of &lt;a href=\"power%20law\"&gt;power law&lt;/a&gt;).\nDefinition for wide-sense stationary stochastic process.\nIf formula_1 is a &lt;a href=\"wide-sense%20stationary%20process\"&gt;wide-sense stationary process&lt;/a&gt; then the mean formula_16 and the variance formula_17 are time-independent, and further the autocovariance function depends only on the lag between formula_10 and formula_11: the autocovariance depends only on the time-distance between the pair of values but not on their position in time. This further implies that the autocovariance and auto-correlation can be expressed as a function of the time-lag, and that this would be an &lt;a href=\"even%20function\"&gt;even function&lt;/a&gt; of the lag formula_20. This gives the more familiar forms for the auto-correlation function\nand the auto-covariance function:\nNormalization.\nIt is common practice in some disciplines (e.g. statistics and &lt;a href=\"time%20series%20analysis\"&gt;time series analysis&lt;/a&gt;) to normalize the autocovariance function to get a time-dependent &lt;a href=\"Pearson%20correlation%20coefficient\"&gt;Pearson correlation coefficient&lt;/a&gt;. However, in other disciplines (e.g. engineering) the normalization is usually dropped and the terms \"autocorrelation\" and \"autocovariance\" are used interchangeably.\nThe definition of the auto-correlation coefficient of a stochastic process is\nformula_21\nIf the function formula_22 is well defined, its value must lie in the range formula_23, with 1 indicating perfect correlation and \u22121 indicating perfect &lt;a href=\"anti-correlation\"&gt;anti-correlation&lt;/a&gt;.\nFor a &lt;a href=\"Stationary%20process%23Weak%20or%20wide-sense%20stationarity\"&gt;weak-sense stationarity, wide-sense stationarity&lt;/a&gt; (WSS) process, the definition is\nformula_24\nwhere\nformula_25\nThe normalization is important both because the interpretation of the autocorrelation as a correlation provides a scale-free measure of the strength of &lt;a href=\"statistical%20dependence\"&gt;statistical dependence&lt;/a&gt;, and because the normalization has an effect on the statistical properties of the estimated autocorrelations.\nProperties.\nSymmetry property.\nThe fact that the auto-correlation function formula_26 is an &lt;a href=\"even%20function\"&gt;even function&lt;/a&gt; can be stated as\nformula_27\nrespectively for a WSS process:\nformula_28\nMaximum at zero.\nFor a WSS process:\nformula_29\nNotice that formula_30 is always real.\nCauchy\u2013Schwarz inequality.\nThe &lt;a href=\"Cauchy%E2%80%93Schwarz%20inequality\"&gt;Cauchy\u2013Schwarz inequality&lt;/a&gt;, inequality for stochastic processes:\nformula_31\nAutocorrelation of white noise.\nThe autocorrelation of a continuous-time &lt;a href=\"white%20noise\"&gt;white noise&lt;/a&gt; signal will have a strong peak (represented by a &lt;a href=\"Dirac%20delta%20function\"&gt;Dirac delta function&lt;/a&gt;) at formula_32 and will be exactly formula_33 for all other formula_34.\nWiener\u2013Khinchin theorem.\nThe &lt;a href=\"Wiener%E2%80%93Khinchin%20theorem\"&gt;Wiener\u2013Khinchin theorem&lt;/a&gt; relates the autocorrelation function formula_26 to the &lt;a href=\"spectral%20density\"&gt;power spectral density&lt;/a&gt; formula_36 via the &lt;a href=\"Fourier%20transform\"&gt;Fourier transform&lt;/a&gt;:\nformula_37\nformula_38\nFor real-valued functions, the symmetric autocorrelation function has a real symmetric transform, so the &lt;a href=\"Wiener%E2%80%93Khinchin%20theorem\"&gt;Wiener\u2013Khinchin theorem&lt;/a&gt; can be re-expressed in terms of real cosines only:\nformula_39\nformula_40\nAuto-correlation of random vectors.\nThe (potentially time-dependent) auto-correlation matrix (also called second moment) of a (potentially time-dependent) &lt;a href=\"random%20vector\"&gt;random vector&lt;/a&gt; formula_41 is an formula_42 matrix containing as elements the autocorrelations of all pairs of elements of the random vector formula_43. The autocorrelation matrix is used in various &lt;a href=\"digital%20signal%20processing\"&gt;digital signal processing&lt;/a&gt; algorithms.\nFor a &lt;a href=\"random%20vector\"&gt;random vector&lt;/a&gt; formula_41 containing &lt;a href=\"random%20element\"&gt;random element&lt;/a&gt;s whose &lt;a href=\"expected%20value\"&gt;expected value&lt;/a&gt; and &lt;a href=\"variance\"&gt;variance&lt;/a&gt; exist, the auto-correlation matrix is defined by\nwhere formula_45 denotes transposition and has dimensions formula_42.\nWritten component-wise:\nformula_47\nIf formula_48 is a &lt;a href=\"complex%20random%20vector\"&gt;complex random vector&lt;/a&gt;, the autocorrelation matrix is instead defined by\nformula_49\nHere formula_50 denotes &lt;a href=\"Hermitian%20transpose\"&gt;Hermitian transposition&lt;/a&gt;.\nFor example, if formula_51 is a random vector, then formula_52 is a formula_53 matrix whose formula_54-th entry is formula_55.\nAuto-correlation of deterministic signals.\nIn &lt;a href=\"signal%20processing\"&gt;signal processing&lt;/a&gt;, the above definition is often used without the normalization, that is, without subtracting the mean and dividing by the variance. When the autocorrelation function is normalized by mean and variance, it is sometimes referred to as the autocorrelation coefficient or autocovariance function.\nAuto-correlation of continuous-time signal.\nGiven a &lt;a href=\"Signal%20%28electronics%29\"&gt;signal&lt;/a&gt; formula_60, the continuous autocorrelation formula_61 is most often defined as the continuous &lt;a href=\"cross-correlation\"&gt;cross-correlation&lt;/a&gt; integral of formula_60 with itself, at lag formula_34.\nwhere formula_64 represents the &lt;a href=\"complex%20conjugate\"&gt;complex conjugate&lt;/a&gt; of formula_60. Note that the parameter formula_2 in the integral is a dummy variable and is only necessary to calculate the integral. It has no specific meaning.\nAuto-correlation of discrete-time signal.\nThe discrete autocorrelation formula_67 at lag formula_68 for a discrete-time signal formula_69 is\nThe above definitions work for signals that are square integrable, or square summable, that is, of finite energy. Signals that \"last forever\" are treated instead as random processes, in which case different definitions are needed, based on expected values. For &lt;a href=\"wide-sense-stationary%20random%20process\"&gt;wide-sense-stationary random process&lt;/a&gt;es, the autocorrelations are defined as\nformula_70\nFor processes that are not &lt;a href=\"Stationary%20process\"&gt;stationary&lt;/a&gt;, these will also be functions of formula_2, or formula_72.\nFor processes that are also &lt;a href=\"Ergodic%20process\"&gt;ergodic&lt;/a&gt;, the expectation can be replaced by the limit of a time average. The autocorrelation of an ergodic process is sometimes defined as or equated to\nformula_73\nThese definitions have the advantage that they give sensible well-defined single-parameter results for periodic functions, even when those functions are not the output of stationary ergodic processes.\nAlternatively, signals that \"last forever\" can be treated by a short-time autocorrelation function analysis, using finite time integrals. (See &lt;a href=\"short-time%20Fourier%20transform\"&gt;short-time Fourier transform&lt;/a&gt; for a related process.)\nDefinition for periodic signals.\nIf formula_74 is a continuous periodic function of period formula_75, the integration from formula_76 to formula_77 is replaced by integration over any interval formula_78 of length formula_75:\nformula_80\nwhich is equivalent to\nformula_81\nProperties.\nIn the following, we will describe properties of one-dimensional autocorrelations only, since most properties are easily transferred from the one-dimensional case to the multi-dimensional cases. These properties hold for &lt;a href=\"Stationary%20process%23Weak%20or%20wide-sense%20stationarity\"&gt;wide-sense stationary processes&lt;/a&gt;.\nMulti-dimensional autocorrelation.\nMulti-&lt;a href=\"dimension\"&gt;dimension&lt;/a&gt;al autocorrelation is defined similarly. For example, in &lt;a href=\"Three-dimensional%20space\"&gt;three dimensions&lt;/a&gt; the autocorrelation of a square-summable &lt;a href=\"discrete%20signal\"&gt;discrete signal&lt;/a&gt; would be\nformula_96\nWhen mean values are subtracted from signals before computing an autocorrelation function, the resulting function is usually called an auto-covariance function.\nEfficient computation.\nFor data expressed as a &lt;a href=\"Discrete%20signal\"&gt;discrete&lt;/a&gt; sequence, it is frequently necessary to compute the autocorrelation with high &lt;a href=\"algorithmic%20efficiency\"&gt;computational efficiency&lt;/a&gt;. A &lt;a href=\"brute%20force%20method\"&gt;brute force method&lt;/a&gt; based on the signal processing definition formula_97 can be used when the signal size is small. For example, to calculate the autocorrelation of the real signal sequence formula_98 (i.e. formula_99, and formula_100 for all other values of ) by hand, we first recognize that the definition just given is the same as the \"usual\" multiplication, but with right shifts, where each vertical addition gives the autocorrelation for particular lag values:\nformula_101\nThus the required autocorrelation sequence is formula_102, where formula_103 formula_104 and formula_105 the autocorrelation for other lag values being zero. In this calculation we do not perform the carry-over operation during addition as is usual in normal multiplication. Note that we can halve the number of operations required by exploiting the inherent symmetry of the autocorrelation. If the signal happens to be periodic, i.e. formula_106 then we get a circular autocorrelation (similar to &lt;a href=\"circular%20convolution\"&gt;circular convolution&lt;/a&gt;) where the left and right tails of the previous autocorrelation sequence will overlap and give formula_107 which has the same period as the signal sequence formula_108 The procedure can be regarded as an application of the convolution property of &lt;a href=\"Z-transform\"&gt;Z-transform&lt;/a&gt; of a discrete signal.\nWhile the brute force algorithm is &lt;a href=\"Big%20O%20notation\"&gt;order&lt;/a&gt; , several efficient algorithms exist which can compute the autocorrelation in order . For example, the &lt;a href=\"Wiener%E2%80%93Khinchin%20theorem\"&gt;Wiener\u2013Khinchin theorem&lt;/a&gt; allows computing the autocorrelation from the raw data with two &lt;a href=\"fast%20Fourier%20transform\"&gt;fast Fourier transform&lt;/a&gt;s (FFT):\nformula_109\nwhere IFFT denotes the inverse &lt;a href=\"fast%20Fourier%20transform\"&gt;fast Fourier transform&lt;/a&gt;. The asterisk denotes &lt;a href=\"complex%20conjugate\"&gt;complex conjugate&lt;/a&gt;.\nAlternatively, a multiple correlation can be performed by using brute force calculation for low values, and then progressively binning the data with a &lt;a href=\"logarithm\"&gt;logarithm&lt;/a&gt;ic density to compute higher values, resulting in the same efficiency, but with lower memory requirements.\nEstimation.\nFor a &lt;a href=\"Discrete%20signal\"&gt;discrete&lt;/a&gt; process with known mean and variance for which we observe formula_72 observations formula_111, an estimate of the autocorrelation coefficient may be obtained as\nformula_112\nfor any positive integer formula_113. When the true mean formula_16 and variance formula_17 are known, this estimate is &lt;a href=\"Biased%20estimator\"&gt;unbiased&lt;/a&gt;. If the true mean and &lt;a href=\"variance\"&gt;variance&lt;/a&gt; of the process are not known there are several possibilities:\nThe advantage of estimates of the last type is that the set of estimated autocorrelations, as a function of formula_122, then form a function which is a valid autocorrelation in the sense that it is possible to define a theoretical process having exactly that autocorrelation. Other estimates can suffer from the problem that, if they are used to calculate the variance of a linear combination of the formula_123's, the variance calculated may turn out to be negative.\nRegression analysis.\nIn &lt;a href=\"regression%20analysis\"&gt;regression analysis&lt;/a&gt; using &lt;a href=\"time%20series%20analysis\"&gt;time series data&lt;/a&gt;, autocorrelation in a variable of interest is typically modeled either with an &lt;a href=\"autoregressive%20model\"&gt;autoregressive model&lt;/a&gt; (AR), a &lt;a href=\"moving%20average%20model\"&gt;moving average model&lt;/a&gt; (MA), their combination as an &lt;a href=\"autoregressive-moving-average%20model\"&gt;autoregressive-moving-average model&lt;/a&gt; (ARMA), or an extension of the latter called an &lt;a href=\"autoregressive%20integrated%20moving%20average%20model\"&gt;autoregressive integrated moving average model&lt;/a&gt; (ARIMA). With multiple interrelated data series, &lt;a href=\"vector%20autoregression\"&gt;vector autoregression&lt;/a&gt; (VAR) or its extensions are used.\nIn &lt;a href=\"ordinary%20least%20squares\"&gt;ordinary least squares&lt;/a&gt; (OLS), the adequacy of a model specification can be checked in part by establishing whether there is autocorrelation of the &lt;a href=\"errors%20and%20residuals%20in%20statistics\"&gt;regression residuals&lt;/a&gt;. Problematic autocorrelation of the errors, which themselves are unobserved, can generally be detected because it produces autocorrelation in the observable residuals. (Errors are also known as \"error terms\" in &lt;a href=\"econometrics\"&gt;econometrics&lt;/a&gt;.) Autocorrelation of the errors violates the ordinary least squares assumption that the error terms are uncorrelated, meaning that the &lt;a href=\"Gauss%E2%80%93Markov%20theorem\"&gt;Gauss Markov theorem&lt;/a&gt; does not apply, and that OLS estimators are no longer the Best Linear Unbiased Estimators (&lt;a href=\"BLUE\"&gt;BLUE&lt;/a&gt;). While it does not bias the OLS coefficient estimates, the &lt;a href=\"Standard%20error%20%28statistics%29\"&gt;standard errors&lt;/a&gt; tend to be underestimated (and the &lt;a href=\"T-statistics\"&gt;t-scores&lt;/a&gt; overestimated) when the autocorrelations of the errors at low lags are positive.\nThe traditional test for the presence of first-order autocorrelation is the &lt;a href=\"Durbin%E2%80%93Watson%20statistic\"&gt;Durbin\u2013Watson statistic&lt;/a&gt; or, if the explanatory variables include a lagged dependent variable, &lt;a href=\"Durbin%E2%80%93Watson%20statistic%23Durbin%20h-statistic\"&gt;Durbin's h statistic&lt;/a&gt;. The Durbin-Watson can be linearly mapped however to the Pearson correlation between values and their lags. A more flexible test, covering autocorrelation of higher orders and applicable whether or not the regressors include lags of the dependent variable, is the &lt;a href=\"Breusch%E2%80%93Godfrey%20test\"&gt;Breusch\u2013Godfrey test&lt;/a&gt;. This involves an auxiliary regression, wherein the residuals obtained from estimating the model of interest are regressed on (a) the original regressors and (b) \"k\" lags of the residuals, where 'k' is the order of the test. The simplest version of the test statistic from this auxiliary regression is \"TR\"2, where \"T\" is the sample size and \"R\"2 is the &lt;a href=\"coefficient%20of%20determination\"&gt;coefficient of determination&lt;/a&gt;. Under the null hypothesis of no autocorrelation, this statistic is asymptotically &lt;a href=\"Chi-squared%20distribution\"&gt;distributed as formula_124&lt;/a&gt; with \"k\" degrees of freedom.\nResponses to nonzero autocorrelation include &lt;a href=\"generalized%20least%20squares\"&gt;generalized least squares&lt;/a&gt; and the &lt;a href=\"Newey%20West\"&gt;Newey\u2013West HAC estimator&lt;/a&gt; (Heteroskedasticity and Autocorrelation Consistent).\nIn the estimation of a &lt;a href=\"moving%20average%20model\"&gt;moving average model&lt;/a&gt; (MA), the autocorrelation function is used to determine the appropriate number of lagged error terms to be included. This is based on the fact that for an MA process of order \"q\", we have formula_125, for formula_126, and formula_127, for formula_128.\nSerial dependence.\nSerial dependence is closely linked to the notion of autocorrelation, but represents a distinct concept (see &lt;a href=\"Correlation%20and%20dependence\"&gt;Correlation and dependence&lt;/a&gt;). In particular, it is possible to have serial dependence but no (linear) correlation. In some fields however, the two terms are used as synonyms.\nA &lt;a href=\"time%20series\"&gt;time series&lt;/a&gt; of a &lt;a href=\"random%20variable\"&gt;random variable&lt;/a&gt; has serial dependence if the value at some time formula_2 in the series is &lt;a href=\"Statistical%20independence\"&gt;statistically dependent&lt;/a&gt; on the value at another time formula_130. A series is serially independent if there is no dependence between any pair.\nIf a time series formula_1 is &lt;a href=\"Stationary%20process\"&gt;stationary&lt;/a&gt;, then statistical dependence between the pair formula_132 would imply that there is statistical dependence between all pairs of values at the same lag formula_133."
}