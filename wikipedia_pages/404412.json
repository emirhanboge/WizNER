{
    "id": "404412",
    "revid": "1066716996",
    "url": "https://en.wikipedia.org/wiki?curid=404412",
    "title": "Bayesian statistics",
    "text": "Bayesian statistics is a theory in the field of &lt;a href=\"statistics\"&gt;statistics&lt;/a&gt; based on the &lt;a href=\"Bayesian%20probability\"&gt;Bayesian interpretation of probability&lt;/a&gt; where &lt;a href=\"probability\"&gt;probability&lt;/a&gt; expresses a \"degree of belief\" in an &lt;a href=\"Event%20%28probability%20theory%29\"&gt;event&lt;/a&gt;. The degree of belief may be based on prior knowledge about the event, such as the results of previous experiments, or on personal beliefs about the event. This differs from a number of other &lt;a href=\"Probability%20interpretations\"&gt;interpretations of probability&lt;/a&gt;, such as the &lt;a href=\"Frequentist%20probability\"&gt;frequentist&lt;/a&gt; interpretation that views probability as the &lt;a href=\"Limit%20of%20a%20sequence\"&gt;limit&lt;/a&gt; of the relative frequency of an event after many trials.\nBayesian statistical methods use &lt;a href=\"Bayes%27%20theorem\"&gt;Bayes' theorem&lt;/a&gt; to compute and update probabilities after obtaining new data. Bayes' theorem describes the &lt;a href=\"conditional%20probability\"&gt;conditional probability&lt;/a&gt; of an event based on data as well as prior information or beliefs about the event or conditions related to the event. For example, in &lt;a href=\"Bayesian%20inference\"&gt;Bayesian inference&lt;/a&gt;, Bayes' theorem can be used to estimate the parameters of a &lt;a href=\"probability%20distribution\"&gt;probability distribution&lt;/a&gt; or &lt;a href=\"statistical%20model\"&gt;statistical model&lt;/a&gt;. Since Bayesian statistics treats probability as a degree of belief, Bayes' theorem can directly assign a probability distribution that quantifies the belief to the parameter or set of parameters.\nBayesian statistics is named after &lt;a href=\"Thomas%20Bayes\"&gt;Thomas Bayes&lt;/a&gt;, who formulated a specific case of Bayes' theorem in &lt;a href=\"An%20Essay%20towards%20solving%20a%20Problem%20in%20the%20Doctrine%20of%20Chances\"&gt;a paper&lt;/a&gt; published in 1763. In several papers spanning from the late 18th to the early 19th centuries, &lt;a href=\"Pierre-Simon%20Laplace\"&gt;Pierre-Simon Laplace&lt;/a&gt; developed the Bayesian interpretation of probability. Laplace used methods that would now be considered Bayesian to solve a number of statistical problems. Many Bayesian methods were developed by later authors, but the term was not commonly used to describe such methods until the 1950s. During much of the 20th century, Bayesian methods were viewed unfavorably by many statisticians due to philosophical and practical considerations. Many Bayesian methods required much computation to complete, and most methods that were widely used during the century were based on the frequentist interpretation. However, with the advent of powerful computers and new &lt;a href=\"algorithm\"&gt;algorithm&lt;/a&gt;s like &lt;a href=\"Markov%20chain%20Monte%20Carlo\"&gt;Markov chain Monte Carlo&lt;/a&gt;, Bayesian methods have seen increasing use within statistics in the 21st century.\nBayes' theorem.\nBayes' theorem is used in Bayesian methods to update probabilities, which are degrees of belief, after obtaining new data. Given two events formula_1 and formula_2, the conditional probability of formula_1 given that formula_2 is true is expressed as follows:\nformula_5\nwhere formula_6. Although Bayes' theorem is a fundamental result of &lt;a href=\"probability%20theory\"&gt;probability theory&lt;/a&gt;, it has a specific interpretation in Bayesian statistics. In the above equation, formula_1 usually represents a &lt;a href=\"proposition\"&gt;proposition&lt;/a&gt; (such as the statement that a coin lands on heads fifty percent of the time) and formula_2 represents the evidence, or new data that is to be taken into account (such as the result of a series of coin flips). formula_9 is the &lt;a href=\"prior%20probability\"&gt;prior probability&lt;/a&gt; of formula_1 which expresses one's beliefs about formula_1 before evidence is taken into account. The prior probability may also quantify prior knowledge or information about formula_1. formula_13 is the &lt;a href=\"likelihood%20function\"&gt;likelihood function&lt;/a&gt;, which can be interpreted as the probability of the evidence formula_2 given that formula_1 is true. The likelihood quantifies the extent to which the evidence formula_2 supports the proposition formula_1. formula_18 is the &lt;a href=\"posterior%20probability\"&gt;posterior probability&lt;/a&gt;, the probability of the proposition formula_1 after taking the evidence formula_2 into account. Essentially, Bayes' theorem updates one's prior beliefs formula_9 after considering the new evidence formula_2.\nThe probability of the evidence formula_23 can be calculated using the &lt;a href=\"law%20of%20total%20probability\"&gt;law of total probability&lt;/a&gt;. If formula_24 is a &lt;a href=\"Partition%20of%20a%20set\"&gt;partition&lt;/a&gt; of the &lt;a href=\"sample%20space\"&gt;sample space&lt;/a&gt;, which is the set of all &lt;a href=\"Outcome%20%28probability%29\"&gt;outcomes&lt;/a&gt; of an experiment, then,\nformula_25\nWhen there are an infinite number of outcomes, it is necessary to &lt;a href=\"Integral\"&gt;integrate&lt;/a&gt; over all outcomes to calculate formula_23 using the law of total probability. Often, formula_23 is difficult to calculate as the calculation would involve sums or integrals that would be time-consuming to evaluate, so often only the product of the prior and likelihood is considered, since the evidence does not change in the same analysis. The posterior is proportional to this product:\nformula_28\nThe &lt;a href=\"maximum%20a%20posteriori\"&gt;maximum a posteriori&lt;/a&gt;, which is the &lt;a href=\"Mode%20%28statistics%29\"&gt;mode&lt;/a&gt; of the posterior and is often computed in Bayesian statistics using &lt;a href=\"mathematical%20optimization\"&gt;mathematical optimization&lt;/a&gt; methods, remains the same. The posterior can be approximated even without computing the exact value of formula_23 with methods such as Markov chain Monte Carlo or &lt;a href=\"variational%20Bayesian%20methods\"&gt;variational Bayesian methods&lt;/a&gt;.\nOutline of Bayesian methods.\nThe general set of statistical techniques can be divided into a number of activities, many of which have special Bayesian versions.\nBayesian inference.\nBayesian inference refers to &lt;a href=\"statistical%20inference\"&gt;statistical inference&lt;/a&gt; where uncertainty in inferences is quantified using probability. In classical &lt;a href=\"frequentist%20inference\"&gt;frequentist inference&lt;/a&gt;, model &lt;a href=\"parameter\"&gt;parameter&lt;/a&gt;s and hypotheses are considered to be fixed. Probabilities are not assigned to parameters or hypotheses in frequentist inference. For example, it would not make sense in frequentist inference to directly assign a probability to an event that can only happen once, such as the result of the next flip of a fair coin. However, it would make sense to state that the proportion of heads &lt;a href=\"Law%20of%20large%20numbers\"&gt;approaches one-half&lt;/a&gt; as the number of coin flips increases.\n&lt;a href=\"Statistical%20models\"&gt;Statistical models&lt;/a&gt; specify a set of statistical assumptions and processes that represent how the sample data are generated. Statistical models have a number of parameters that can be modified. For example, a coin can be represented as samples from a &lt;a href=\"Bernoulli%20distribution\"&gt;Bernoulli distribution&lt;/a&gt;, which models two possible outcomes. The Bernoulli distribution has a single parameter equal to the probability of one outcome, which in most cases is the probability of landing on heads. Devising a good model for the data is central in Bayesian inference. In most cases, models only approximate the true process, and may not take into account certain factors influencing the data. In Bayesian inference, probabilities can be assigned to model parameters. Parameters can be represented as &lt;a href=\"random%20variable\"&gt;random variable&lt;/a&gt;s. Bayesian inference uses Bayes' theorem to update probabilities after more evidence is obtained or known.\nStatistical modeling.\nThe formulation of &lt;a href=\"statistical%20model\"&gt;statistical model&lt;/a&gt;s using Bayesian statistics has the identifying feature of requiring the specification of &lt;a href=\"prior%20distribution\"&gt;prior distribution&lt;/a&gt;s for any unknown parameters. Indeed, parameters of prior distributions may themselves have prior distributions, leading to &lt;a href=\"Bayesian%20hierarchical%20modeling\"&gt;Bayesian hierarchical modeling&lt;/a&gt;, or may be interrelated, leading to &lt;a href=\"Bayesian%20networks\"&gt;Bayesian networks&lt;/a&gt;.\nDesign of experiments.\nThe &lt;a href=\"Bayesian%20design%20of%20experiments\"&gt;Bayesian design of experiments&lt;/a&gt; includes a concept called 'influence of prior beliefs'. This approach uses &lt;a href=\"sequential%20analysis\"&gt;sequential analysis&lt;/a&gt; techniques to include the outcome of earlier experiments in the design of the next experiment. This is achieved by updating 'beliefs' through the use of prior and &lt;a href=\"posterior%20distribution\"&gt;posterior distribution&lt;/a&gt;. This allows the design of experiments to make good use of resources of all types. An example of this is the &lt;a href=\"multi-armed%20bandit%20problem\"&gt;multi-armed bandit problem&lt;/a&gt;.\nExploratory analysis of Bayesian models.\nExploratory analysis of Bayesian models is an adaptation or extension of the &lt;a href=\"exploratory%20data%20analysis\"&gt;exploratory data analysis&lt;/a&gt; approach to the needs and peculiarities of Bayesian modeling. In the words of Persi Diaconis: \nThe &lt;a href=\"Bayesian%20inference\"&gt;inference process&lt;/a&gt; generates a posterior distribution, which has a central role in Bayesian statistics, together with other distributions like the posterior predictive distribution and the prior predictive distribution. The correct visualization, analysis, and interpretation of these distributions is key to properly answer the questions that motivate the inference process.\nWhen working with Bayesian models there are a series of related tasks that need to be addressed besides inference itself:\nAll these tasks are part of the Exploratory analysis of Bayesian models approach and successfully performing them is central to the iterative and interactive modeling process. These tasks require both numerical and visual summaries."
}